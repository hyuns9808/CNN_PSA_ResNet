{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN attack via ResNet using Skywater non-linearized data\n",
    "\n",
    "The goal of this notebook is to correctly preprocess the given data as tensors that can be used to train ResNet101."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader.py\n",
    "# Necessary imports\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataloaders\n",
    "Used files from Kareem's GitHub repo. I may have made mistakes in sampling the data, so feel free to change anything that has been configured incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader.py\n",
    "# Returns list of files with given format \n",
    "def get_files(directory, format, digital_index=0):\n",
    "\n",
    "    format = re.compile(format)\n",
    "    files = os.listdir(directory)\n",
    "\n",
    "    #file_dict = {}\n",
    "    file_list = [] # fname, fpath, label\n",
    "\n",
    "    for fname in files:\n",
    "        if match := format.match(fname):\n",
    "            fpath = os.path.join(directory, fname)\n",
    "\n",
    "            dvalue = int(match.groups()[digital_index])\n",
    "            \n",
    "            file_list.append((fname, fpath, dvalue))\n",
    "\n",
    "            #if dvalue in file_dict:\n",
    "            #    file_dict[dvalue].append(fpath)\n",
    "            #else:\n",
    "            #    file_dict[dvalue] = [fpath]\n",
    "\n",
    "    return file_list #file_dict, file_path\n",
    "\n",
    "# Creates dataset with given traces\n",
    "class TraceDataset(Dataset):\n",
    "    cached_traces = {}\n",
    "    trace_list    = []\n",
    "\n",
    "    def __init__(self, file_list, cache=True):\n",
    "        self.file_list = file_list\n",
    "        self.cache     = cache\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        fname, fpath, label = self.file_list[index]\n",
    "        label = self.process_label(label)\n",
    "\n",
    "        if self.cache and fname in self.cached_traces:\n",
    "            return self.cached_traces[fname], label\n",
    "        else:\n",
    "            return self.load_trace(fname, fpath), label\n",
    "\n",
    "    def get_info(self, index):\n",
    "        return self.file_list[index]\n",
    "\n",
    "    def load_trace(self, fname, fpath):\n",
    "        with open(fpath, 'r') as file:\n",
    "            header = file.readline()\n",
    "            #time_arr = []\n",
    "            valu_arr = []\n",
    "\n",
    "            for line in file.readlines():\n",
    "                time, value = line.strip().split()\n",
    "                #time_arr.append(np.float32(time))\n",
    "                valu_arr.append(np.float32(value))\n",
    "\n",
    "        trace = np.array(valu_arr, dtype=np.float32)\n",
    "\n",
    "        if self.cache: \n",
    "            self.cached_traces[fname] = trace\n",
    "            self.trace_list.append(trace)\n",
    "\n",
    "        return trace\n",
    "    \n",
    "    def process_label(self, label): return label\n",
    "\n",
    "    def cache_all(self):\n",
    "        assert self.cache == True\n",
    "\n",
    "        print(\"Caching all traces\")\n",
    "        for fname, fpath, label in self.file_list:\n",
    "            self.load_trace(fname, fpath)\n",
    "        print(\"DONE Caching all traces\")\n",
    "\n",
    "class TraceDatasetBW(TraceDataset):\n",
    "    def __init__(self, file_list, bit_select, cache=True):\n",
    "        self.bit_mask = 1 << bit_select\n",
    "        super().__init__(file_list, cache=cache)\n",
    "\n",
    "    def process_label(self, label):\n",
    "        return 1 if label & self.bit_mask else 0\n",
    "\n",
    "class TraceDatasetBuilder:\n",
    "    def __init__(self, adc_bitwidth=8, cache=True):\n",
    "        self.file_list        = []\n",
    "        self.cache = cache\n",
    "        self.adc_bits = adc_bitwidth\n",
    "\n",
    "        self.dataset = None\n",
    "        self.dataloader = None\n",
    "        self.datasets = []\n",
    "        self.dataloaders = []\n",
    "\n",
    "    def add_files(self, directory, format, label_group):\n",
    "        ''' Builds list of powertrace files\n",
    "        Inputs:\n",
    "            directory   : folder to search for files\n",
    "            format      : regular expression to match filenames\n",
    "            label_index : group index for digital output label corresponding to trace\n",
    "        Outputs:\n",
    "            list        : [(file_name, file_path, label) ... ]\n",
    "        '''\n",
    "        format = re.compile(format)\n",
    "        fnames = os.listdir(directory)\n",
    "\n",
    "        for fname in fnames:\n",
    "            if match := format.match(fname):\n",
    "                fpath = os.path.join(directory, fname)\n",
    "                dvalue = int(match.groups()[label_group])\n",
    "\n",
    "                self.file_list.append((fname, fpath, dvalue))\n",
    "\n",
    "    def build(self):\n",
    "        self.dataset = TraceDataset(self.file_list, cache=self.cache)\n",
    "        for b in range(self.adc_bits):\n",
    "            self.datasets.append(TraceDatasetBW(self.file_list, b, cache=self.cache))\n",
    "\n",
    "        if self.cache:\n",
    "            self.dataset.cache_all()\n",
    "\n",
    "    def build_dataloaders(self, **kwargs): # batch_size=256, shuffle=True\n",
    "        self.dataloader = DataLoader(self.dataset, **kwargs)\n",
    "        self.dataloaders = [DataLoader(dataset, **kwargs) for dataset in self.datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching all traces\n",
      "DONE Caching all traces\n"
     ]
    }
   ],
   "source": [
    "# Create dataloaders\n",
    "pwd = os.getcwd()\n",
    "# print(pwd)\n",
    "# proj_dir = os.path.dirname(os.path.dirname(pwd))\n",
    "# print(proj_dir)\n",
    "# data_dir = os.path.join(pwd, 'analog', 'outfiles')\n",
    "\n",
    "builder  = TraceDatasetBuilder(adc_bitwidth=8, cache=True)\n",
    "builder.add_files(os.path.join(pwd, 'sky_Dec_18_2151'), \"sky_d(\\\\d+)_.*\\\\.txt\", 0)\n",
    "builder.build()\n",
    "builder.build_dataloaders(batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for ResNet\n",
    "Section where the initial imports and variables for ResNet is set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet18 code\n",
    "# Necessary imports\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "import numpy as np\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing CUDA version\n",
    "Section where we desginate the most updated CUDA version for current GPU.\n",
    "\n",
    "Implementation in progress, but is a lot of unnecessary work; will do if CPU training speeds are unformidable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training CNNs - ResNet18\n",
    "Using pretrained ResNet18, we train each CNN until all of them reaches an accuracy of 1.\n",
    "\n",
    "I'm not sure if aiming for an accuracy of 1 is beneficial, as it is just overfitting the model to the training data. A more realistic value may be 0.99, but will set it to 1 for the current simulated environment.\n",
    "\n",
    "The training function automatically reduces the learning rate used in Adam based on target accuracy. Currently testing different values and decrease rates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Function that initializes the CNN and its core features.\n",
    "Inputs:\n",
    "    None\n",
    "Returns:\n",
    "    1) cnn: model used(ResNet18)\n",
    "    2) criterion: loss function, current default=CrossEntropyLoss\n",
    "    3) learning_rate: learning rate, current default=1e-4\n",
    "    4) optimizer: optimizer, current default=Adam\n",
    "'''\n",
    "def cnn_init():\n",
    "    # Model: ResNet101, pretrained=true, using ResNet101_Weights.DEFAULT for up-to-date values\n",
    "    cnn = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "    # Freeze all layers\n",
    "    for param in cnn.parameters():\n",
    "        param.requires_grad = False\n",
    "    # Resulting output is either 0 or 1\n",
    "    cnn.fc = nn.Linear(cnn.fc.in_features, 2)\n",
    "    # Loss function: not specified in paper, using Cross Entropy Loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Learning rate: default set to 1e-4, adjust accordingly\n",
    "    # Optimizer: not specified in paper, using Adam\n",
    "    learning_rate = 1e-4\n",
    "    optimizer = optim.Adam(cnn.parameters(), lr=learning_rate)\n",
    "    return cnn, criterion, learning_rate, optimizer\n",
    "\n",
    "''' \n",
    "Function that initializes parameters used in training.\n",
    "Inputs:\n",
    "    None\n",
    "Returns:\n",
    "    TARGET ACCURACY PARAMETERS\n",
    "    1) target_acc: array with target accuracies which reached updates the learning rate\n",
    "    2) target_acc_index: value used to track progress of target accuracy\n",
    "    OSCILLATION CHECKING PARAMETERS\n",
    "    3) is_osc: boolean value that is set to true when oscillation is detected\n",
    "    4) osc_count: value that counts number of similar accuracy outputs to check oscillation\n",
    "    TRAINING PARAMETERS\n",
    "    5) num_epochs: number of maximum epochs per training\n",
    "    6) max_grad_norm: gradient clipping threshold\n",
    "'''\n",
    "def param_init():\n",
    "    # target accuracy parameters\n",
    "    target_acc = [0.90, 0.95, 0.99, 0.995, 1.0]\n",
    "    target_acc_index = 0\n",
    "    \n",
    "    # oscillation checking parameters\n",
    "    is_osc = False\n",
    "    osc_count = 0\n",
    "\n",
    "    # training parameters\n",
    "    num_epochs = 1000\n",
    "    max_grad_norm = 1.0\n",
    "    \n",
    "    return target_acc, target_acc_index, is_osc, osc_count, num_epochs, max_grad_norm\n",
    "\n",
    "''' \n",
    "Function that loads variables from saved .pth file. Checks if the file is valid.\n",
    "This function simply receives the checkpoint file and checks if it is valid.\n",
    "Inputs:\n",
    "    1) checkpoint_path: string of .pth file name \n",
    "    2) device: selected device to run training\n",
    "Returns:\n",
    "    1) checkpoint: loaded .pth file contents\n",
    "'''\n",
    "def load_pth_file(checkpoint_path, device):\n",
    "    # Load .pth file\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)\n",
    "        start_epoch = checkpoint.get('epoch', 0)\n",
    "        reached_target_acc = checkpoint.get('reached_acc', 0)\n",
    "        prev_acc = checkpoint.get('prev_acc', 0)\n",
    "        osc_count = checkpoint.get('osc_count', 0)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"No checkpoint found at {checkpoint_path}. Starting from scratch.\")\n",
    "    except KeyError as e:\n",
    "        missing_keys = {\n",
    "            'epoch': \"epoch\",\n",
    "            'prev_acc': \"accuracy of previos epoch\",\n",
    "            'reached_acc': \"reached accuracy\",\n",
    "            'osc_count': \"oscillation count\",\n",
    "            'cnn_state_dict': \"CNN state dictionary\",\n",
    "            'optimizer_state_dict': \"optimizer state dictionary\"\n",
    "        }\n",
    "        key = e.args[0]\n",
    "        if key in missing_keys:\n",
    "            if key in ['cnn_state_dict', 'optimizer_state_dict']:\n",
    "                print(f\"Critical error: Missing \\\"{missing_keys[key]}\\\". Checkpoint file is invalid.\")\n",
    "                raise\n",
    "            else:\n",
    "                print(f\"Warning: Missing \\\"{missing_keys[key]}\\\". Default values will be used.\")\n",
    "                if key == 'reached_acc':\n",
    "                    reached_acc = 0\n",
    "                elif key == 'prev_acc':\n",
    "                    prev_acc = 0\n",
    "                elif key == 'osc_count':\n",
    "                    osc_count = 0\n",
    "\n",
    "        print(f\"Unknown parameter \\\"{key}\\\" in checkpoint. Recheck file and try again.\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def set_learning_rate():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'if' statement on line 81 (775820481.py, line 83)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 83\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(f\"Unknown parameter \\\"{key}\\\" in checkpoint. Recheck file and try again.\")\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'if' statement on line 81\n"
     ]
    }
   ],
   "source": [
    "cnns = []\n",
    "dataloaders = builder.dataloaders \n",
    "timestamp   = datetime.datetime.now().strftime('%Y%m%d_%H%M')\n",
    "\n",
    "print(f\"Installed CUDA version: {torch.version.cuda}\\n\")\n",
    "\n",
    "# Create CNN per dataset\n",
    "for i in range(7,-1,-1):\n",
    "    print(f\"Starting training for \\\"cnn_{i}\\\"...\")\n",
    "\n",
    "    # initialize CNN\n",
    "    cnn, criterion, learning_rate, optimizer = cnn_init()\n",
    "    # initialize parameters\n",
    "    target_acc, target_acc_index, is_osc, osc_count, num_epochs, max_grad_norm = param_init()\n",
    "\n",
    "    # Append CNN to cnns array\n",
    "    cnns.append(cnn)\n",
    "\n",
    "    # Create checkpoint to save progress\n",
    "    checkpoint_path = f\"resnet18_checkpoint_{i}.pth\"\n",
    "    start_epoch = 0\n",
    "    \n",
    "    # Set device to cuda if available\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"\\tGPU found, running training on GPU...\")\n",
    "        device = torch.device(\"cuda\")\n",
    "        cnn = cnn.to(device)\n",
    "    else:\n",
    "        print(\"\\tNo GPU found, running training on CPU...\")\n",
    "        print(\"\\tRecheck CUDA version and if your GPU supports it.\")\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    # Attempt to load saved .pth file\n",
    "    checkpoint = load_pth_file(checkpoint_path, device)\n",
    "\n",
    "    # Load .pth file\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)\n",
    "\n",
    "        cnn.load_state_dict(checkpoint['cnn_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "        start_epoch = checkpoint.get('epoch', 0)\n",
    "        reached_target_acc = checkpoint.get('reached_acc', 0)\n",
    "        prev_acc = checkpoint.get('prev_acc', 0)\n",
    "        osc_count = checkpoint.get('osc_count', 0)\n",
    "\n",
    "        print(f\"Checkpoint loaded. Resuming from epoch {start_epoch}\")\n",
    "        print(f\"\\tPrevious reached accuracy: {reached_target_acc}\")\n",
    "        \n",
    "        # Skip training if target accuracy is reached\n",
    "        if reached_target_acc == 1:\n",
    "            print(f\"\\tSkipping training: accuracy of 1 already achieved.\\n\")\n",
    "            continue\n",
    "\n",
    "        # Adjust target accuracy and learning rate\n",
    "        target_acc_index = 0\n",
    "        while target_acc_index < len(target_acc) - 1 and target_acc[target_acc_index] < reached_target_acc:\n",
    "            target_acc_index += 1\n",
    "            learning_rate /= 2\n",
    "\n",
    "        print(f\"\\tUpdated target accuracy: {target_acc[target_acc_index]}\")\n",
    "        print(f\"\\tUpdated learning rate: {learning_rate}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"No checkpoint found at {checkpoint_path}. Starting from scratch.\")\n",
    "\n",
    "    except KeyError as e:\n",
    "        missing_keys = {\n",
    "            'epoch': \"epoch\",\n",
    "            'reached_acc': \"reached accuracy\",\n",
    "            'osc_count': \"oscillation count\",\n",
    "            'prev_acc': \"accuracy of previos epoch\",\n",
    "            'cnn_state_dict': \"CNN state dictionary\",\n",
    "            'optimizer_state_dict': \"optimizer state dictionary\"\n",
    "        }\n",
    "\n",
    "        key = e.args[0]\n",
    "        if key in missing_keys:\n",
    "            if key in ['cnn_state_dict', 'optimizer_state_dict']:\n",
    "                print(f\"Critical error: Missing \\\"{missing_keys[key]}\\\". Checkpoint file is invalid.\")\n",
    "                raise\n",
    "            else:\n",
    "                print(f\"Warning: Missing \\\"{missing_keys[key]}\\\". Default values will be used.\")\n",
    "                if key == 'reached_acc':\n",
    "                    reached_acc = 0\n",
    "                if key == 'prev_acc':\n",
    "                    prev_acc = 0\n",
    "                elif key == 'osc_count':\n",
    "                    osc_count = 0\n",
    "\n",
    "        print(f\"Unknown parameter \\\"{key}\\\" in checkpoint. Recheck file and try again.\")\n",
    "        raise\n",
    "\n",
    "    # Start training\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        correct = 0\n",
    "        cnn.train()\n",
    "\n",
    "        for inputs, labels in dataloaders[i]:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # Add dimensions for channels and width\n",
    "            inputs = inputs.unsqueeze(1).unsqueeze(-1)\n",
    "            optimizer.zero_grad()\n",
    "            output = cnn(inputs)\n",
    "            # Check for NaN in outputs\n",
    "            if torch.isnan(output).any():\n",
    "                print(\"NaN detected in cnn outputs.\")\n",
    "                break\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "            # Check for NaN in loss\n",
    "            if torch.isnan(loss):\n",
    "                print(\"NaN detected in loss. Stopping training.\")\n",
    "                break\n",
    "            # print(f\"Loss: {loss.item()}\")\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(cnn.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            correct += (predicted == labels).sum()\n",
    "        \n",
    "        accuracy = correct / 256\n",
    "        '''\n",
    "        acc_arr.append(accuracy)\n",
    "        loss_arr.append(loss.item())\n",
    "        '''\n",
    "        # Change rate of update for printing accuracy accordingly\n",
    "        if (epoch + 1) % 1 == 0:\n",
    "            print(f'TRAINING: cnn[{i}], Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "            print(f'TRAINING: cnn[{i}], Epoch {epoch+1}, Accuracy: {accuracy}')\n",
    "        '''\n",
    "        if epoch % 50 == 0: \n",
    "            if loss_g: loss_g.remove()\n",
    "            if acc_g:  acc_g.remove()\n",
    "            loss_g = axs[0].plot(loss_arr, color='lightgray', linestyle='dotted')[0]\n",
    "            loss_a = axs[1].plot(acc_arr,  color='lightgray', linestyle='dotted')[0]\n",
    "            plt.pause(0.01)\n",
    "        '''\n",
    "\n",
    "        # Save checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'cnn_state_dict': cnn.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'reached_acc': accuracy,\n",
    "            'osc_count': osc_count,\n",
    "            'prev_acc': \n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved for epoch {epoch + 1}\")\n",
    "\n",
    "        # Check oscillation\n",
    "        if osc_count > 5:\n",
    "            print(\"OSCILLATION DETECTED. Resetting learning rate...\")\n",
    "        \n",
    "        # If training reached accuracy of 1, stop training\n",
    "        if accuracy == 1:\n",
    "            print(f\"Reached accuracy of 1. Stopping training for \\\"cnn_{i}\\\".\\n\")\n",
    "            break\n",
    "        # Update learning rate if accuracy reaches target value\n",
    "        # Reducing stepsize accordingly so the optimizer does not overshoot\n",
    "        elif target_acc[target_acc_index] < accuracy and target_acc_index != len(target_acc) - 1:\n",
    "            updated = False\n",
    "            temp = learning_rate\n",
    "            while target_acc_index <= len(target_acc) - 1 and target_acc[target_acc_index] < accuracy:\n",
    "                target_acc_index += 1\n",
    "                learning_rate = learning_rate / 2\n",
    "                updated = True\n",
    "            if updated:\n",
    "                print(f\"Reached target accuracy {target_acc[target_acc_index-1]}\")\n",
    "                print(f\"New target accuracy {target_acc[target_acc_index]}\")\n",
    "                print(f\"Updating learning rate FROM: {temp}, TO: {learning_rate}\")\n",
    "\n",
    "'''\n",
    "    label = f'cnn[{i}]'\n",
    "    axs[0].plot(loss_arr, label=label)\n",
    "    axs[1].plot(acc_arr,  label=label)\n",
    "    axs[0].legend()\n",
    "    axs[1].legend()\n",
    "    plt.pause(0.01)\n",
    "\n",
    "plt.pause(60*10)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the CNNs\n",
    "\n",
    "Using the resulting CNNs saved in the .pth files, we test each CNNs using the provided power traces.\n",
    "\n",
    "Currently the testing data is a subset of the training data. In the future, if we can generate more data, we will be able to use separate datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded. Resuming from epoch 7\n",
      "Previous reached accuracy: 1.0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo checkpoint found for \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mcnn_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m. Starting from scratch.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m cnns[i]\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     19\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     20\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Run evaluation using testing data\n",
    "# CURRENTLY USING TRAINING DATA TO TEST DATA: use separate dataset in the future\n",
    "for i in range(7,-1,-1):\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)\n",
    "        cnn.load_state_dict(checkpoint['cnn_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        print(f\"Checkpoint loaded. Resuming from epoch {start_epoch}\")\n",
    "        reached_target_acc = checkpoint['reached_acc']\n",
    "        print(f\"Previous reached accuracy: {reached_target_acc}\")\n",
    "        if reached_target_acc != 1:\n",
    "            print(f\"cnn_{i}\\\" did not reach accuracy of 1, skipping evaluation.\\n\\n\")\n",
    "            continue\n",
    "    except FileNotFoundError:\n",
    "        print(f\"No checkpoint found for \\\"cnn_{i}\\\". Starting from scratch.\")\n",
    "\n",
    "    cnns[i].eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloaders[i]:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # Add dimensions for channels and width\n",
    "            inputs = inputs.unsqueeze(1).unsqueeze(-1)\n",
    "            outputs = cnns[i](inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Accuracy: {100 * correct / total:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
