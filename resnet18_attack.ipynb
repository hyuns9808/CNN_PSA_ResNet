{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN attack via ResNet using Skywater non-linearized data\n",
    "\n",
    "The goal of this notebook is to correctly preprocess the given data as tensors that can be used to train ResNet101."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader.py\n",
    "# Necessary imports\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataloaders\n",
    "Used files from Kareem's GitHub repo. I may have made mistakes in sampling the data, so feel free to change anything that has been configured incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader.py\n",
    "# Returns list of files with given format \n",
    "def get_files(directory, format, digital_index=0):\n",
    "\n",
    "    format = re.compile(format)\n",
    "    files = os.listdir(directory)\n",
    "\n",
    "    #file_dict = {}\n",
    "    file_list = [] # fname, fpath, label\n",
    "\n",
    "    for fname in files:\n",
    "        if match := format.match(fname):\n",
    "            fpath = os.path.join(directory, fname)\n",
    "\n",
    "            dvalue = int(match.groups()[digital_index])\n",
    "            \n",
    "            file_list.append((fname, fpath, dvalue))\n",
    "\n",
    "            #if dvalue in file_dict:\n",
    "            #    file_dict[dvalue].append(fpath)\n",
    "            #else:\n",
    "            #    file_dict[dvalue] = [fpath]\n",
    "\n",
    "    return file_list #file_dict, file_path\n",
    "\n",
    "# Creates dataset with given traces\n",
    "class TraceDataset(Dataset):\n",
    "    cached_traces = {}\n",
    "    trace_list    = []\n",
    "\n",
    "    def __init__(self, file_list, cache=True):\n",
    "        self.file_list = file_list\n",
    "        self.cache     = cache\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        fname, fpath, label = self.file_list[index]\n",
    "        label = self.process_label(label)\n",
    "\n",
    "        if self.cache and fname in self.cached_traces:\n",
    "            return self.cached_traces[fname], label\n",
    "        else:\n",
    "            return self.load_trace(fname, fpath), label\n",
    "\n",
    "    def get_info(self, index):\n",
    "        return self.file_list[index]\n",
    "\n",
    "    def load_trace(self, fname, fpath):\n",
    "        with open(fpath, 'r') as file:\n",
    "            header = file.readline()\n",
    "            #time_arr = []\n",
    "            valu_arr = []\n",
    "            # Fixed error of float32 incorrectly translating values\n",
    "            for line in file.readlines():\n",
    "                time, value = line.strip().split()\n",
    "                try:\n",
    "                    match = re.search(r\"(?<=e-)\\d+\", value)\n",
    "                    if match:\n",
    "                        if value[0] == \"-\":\n",
    "                            strip_val = value[0:11]\n",
    "                        else:\n",
    "                            strip_val = value[0:10]\n",
    "                    float_val = np.float64(strip_val)\n",
    "                    rounded_val = round(float_val, 7)\n",
    "                    valu_arr.append(np.float32(rounded_val))\n",
    "\n",
    "                except ValueError as e:\n",
    "                    print(f\"Error parsing value '{value}': {e}\")\n",
    "\n",
    "        trace = np.array(valu_arr, dtype=np.float32)\n",
    "\n",
    "        if self.cache: \n",
    "            self.cached_traces[fname] = trace\n",
    "            self.trace_list.append(trace)\n",
    "\n",
    "        return trace\n",
    "    \n",
    "    def process_label(self, label): return label\n",
    "\n",
    "    def cache_all(self):\n",
    "        assert self.cache == True\n",
    "\n",
    "        print(\"Caching all traces\")\n",
    "        for fname, fpath, label in self.file_list:\n",
    "            self.load_trace(fname, fpath)\n",
    "        print(\"DONE Caching all traces\")\n",
    "\n",
    "class TraceDatasetBW(TraceDataset):\n",
    "    def __init__(self, file_list, bit_select, cache=True):\n",
    "        self.bit_mask = 1 << bit_select\n",
    "        super().__init__(file_list, cache=cache)\n",
    "\n",
    "    def process_label(self, label):\n",
    "        return 1 if label & self.bit_mask else 0\n",
    "\n",
    "class TraceDatasetBuilder:\n",
    "    def __init__(self, adc_bitwidth=8, cache=True):\n",
    "        self.file_list        = []\n",
    "        self.cache = cache\n",
    "        self.adc_bits = adc_bitwidth\n",
    "\n",
    "        self.dataset = None\n",
    "        self.dataloader = None\n",
    "        self.datasets = []\n",
    "        self.dataloaders = []\n",
    "\n",
    "    def add_files(self, directory, format, label_group):\n",
    "        ''' Builds list of powertrace files\n",
    "        Inputs:\n",
    "            directory   : folder to search for files\n",
    "            format      : regular expression to match filenames\n",
    "            label_index : group index for digital output label corresponding to trace\n",
    "        Outputs:\n",
    "            list        : [(file_name, file_path, label) ... ]\n",
    "        '''\n",
    "        format = re.compile(format)\n",
    "        fnames = os.listdir(directory)\n",
    "\n",
    "        for fname in fnames:\n",
    "            if match := format.match(fname):\n",
    "                fpath = os.path.join(directory, fname)\n",
    "                dvalue = int(match.groups()[label_group])\n",
    "\n",
    "                self.file_list.append((fname, fpath, dvalue))\n",
    "\n",
    "    def build(self):\n",
    "        self.dataset = TraceDataset(self.file_list, cache=self.cache)\n",
    "        for b in range(self.adc_bits):\n",
    "            self.datasets.append(TraceDatasetBW(self.file_list, b, cache=self.cache))\n",
    "\n",
    "        if self.cache:\n",
    "            self.dataset.cache_all()\n",
    "\n",
    "    def build_dataloaders(self, **kwargs): # batch_size=256, shuffle=True\n",
    "        self.dataloader = DataLoader(self.dataset, **kwargs)\n",
    "        self.dataloaders = [DataLoader(dataset, **kwargs) for dataset in self.datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/calya/Desktop/PowerTraces/CNN_PSA_ResNet/sky_Dec_18_2151'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# print(pwd)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# proj_dir = os.path.dirname(os.path.dirname(pwd))\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# print(proj_dir)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# data_dir = os.path.join(pwd, 'analog', 'outfiles')\u001b[39;00m\n\u001b[1;32m      8\u001b[0m builder  \u001b[38;5;241m=\u001b[39m TraceDatasetBuilder(adc_bitwidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 9\u001b[0m builder\u001b[38;5;241m.\u001b[39madd_files(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(pwd, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msky_Dec_18_2151\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msky_d(\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124md+)_.*\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     10\u001b[0m builder\u001b[38;5;241m.\u001b[39mbuild()\n\u001b[1;32m     11\u001b[0m builder\u001b[38;5;241m.\u001b[39mbuild_dataloaders(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[2], line 108\u001b[0m, in \u001b[0;36mTraceDatasetBuilder.add_files\u001b[0;34m(self, directory, format, label_group)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m''' Builds list of powertrace files\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03mInputs:\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    directory   : folder to search for files\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    list        : [(file_name, file_path, label) ... ]\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m--> 108\u001b[0m fnames \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(directory)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fname \u001b[38;5;129;01min\u001b[39;00m fnames:\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m match \u001b[38;5;241m:=\u001b[39m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mmatch(fname):\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/calya/Desktop/PowerTraces/CNN_PSA_ResNet/sky_Dec_18_2151'"
     ]
    }
   ],
   "source": [
    "# Create dataloaders\n",
    "pwd = os.getcwd()\n",
    "# print(pwd)\n",
    "# proj_dir = os.path.dirname(os.path.dirname(pwd))\n",
    "# print(proj_dir)\n",
    "# data_dir = os.path.join(pwd, 'analog', 'outfiles')\n",
    "\n",
    "builder  = TraceDatasetBuilder(adc_bitwidth=8, cache=True)\n",
    "builder.add_files(os.path.join(pwd, 'sky_Dec_18_2151'), \"sky_d(\\\\d+)_.*\\\\.txt\", 0)\n",
    "builder.build()\n",
    "builder.build_dataloaders(batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for ResNet\n",
    "Section where the initial imports and variables for ResNet is set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet18 code\n",
    "# Necessary imports\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "import numpy as np\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing CUDA version\n",
    "Section where we desginate the most updated CUDA version for current GPU.\n",
    "\n",
    "Implementation in progress, but is a lot of unnecessary work; will do if CPU training speeds are unformidable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training CNNs - ResNet18\n",
    "Using pretrained ResNet18, we train each CNN until all of them reaches an accuracy of 1.0.\n",
    "\n",
    "I'm not sure if aiming for an accuracy of 1.0 is beneficial, as it is just overfitting the model to the training data. A more realistic value may be 0.99, but will set it to 1.0 for the current simulated environment.\n",
    "\n",
    "The training function automatically reduces the learning rate used in Adam based on target accuracy. Currently testing different values and decrease rates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "1) def_lr: Default learning rate. Change this value to test different learning rate values.\n",
    "2) divide_lr: Rate of how much the learning rate should be reduced if weight decay is enabled. Increase this value to decrease the learning rate more.\n",
    "3) enable_weight_decay: If set to 'True', enables (1) updating the learning rate according to the resulting accuracy values and (2) oscillation detection (updating the learning rate if there are no improvements over multiple training epochs).\n",
    "4) freeze_layers: If set to 'True', freezes all layers of ResNet with no additional training to those layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main hyperparameters\n",
    "def_lr = 1e-4\n",
    "divide_lr = 2\n",
    "enable_weight_decay = False\n",
    "freeze_layers = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Function that initializes the CNN and its core components.\n",
    "Inputs:\n",
    "    None\n",
    "Returns:\n",
    "    1) cnn: model used(ResNet18)\n",
    "    2) criterion: loss function, current default=CrossEntropyLoss\n",
    "    3) learning_rate: learning rate, current default=def_lr\n",
    "    4) optimizer: optimizer, current default=Adam\n",
    "'''\n",
    "def cnn_init():\n",
    "    # Model: ResNet18, pretrained=true, using ResNet18_Weights.DEFAULT for up-to-date values\n",
    "    cnn = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "    if freeze_layers:\n",
    "        # Freeze all layers\n",
    "        for param in cnn.parameters():\n",
    "            param.requires_grad = False\n",
    "    cnn.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    # Resulting output is either 0 or 1\n",
    "    cnn.fc = nn.Linear(cnn.fc.in_features, 2)\n",
    "    # Loss function: not specified in paper, using Cross Entropy Loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Learning rate: default set to def_lr, adjust accordingly\n",
    "    # Optimizer: not specified in paper, using Adam\n",
    "    learning_rate = def_lr\n",
    "    optimizer = optim.Adam(cnn.parameters(), lr=learning_rate)\n",
    "    return cnn, criterion, learning_rate, optimizer\n",
    "\n",
    "''' \n",
    "Function that initializes parameters used in training.\n",
    "Inputs:\n",
    "    None\n",
    "Returns:\n",
    "    TARGET ACCURACY PARAMETERS\n",
    "    1) target_acc: array with target accuracies which reached updates the learning rate\n",
    "    2) target_acc_index: value used to track progress of target accuracy\n",
    "    OSCILLATION CHECKING PARAMETERS\n",
    "    3) is_osc: boolean value that is set to true when oscillation is detected. Automatically set to False\n",
    "    if new .pth file is loaded.\n",
    "    4) osc_count: value that counts number of similar accuracy outputs to check oscillation\n",
    "    TRAINING PARAMETERS\n",
    "    5) num_epochs: number of maximum epochs per training\n",
    "    6) max_grad_norm: gradient clipping threshold\n",
    "    WEIGHT DECAY PARAMETER\n",
    "    7) weight_decay: set to hyperparameter value 'enable_weight_decay'.\n",
    "    If True, enables weight decay and oscallation update. If False, disables it.\n",
    "'''\n",
    "def param_init():\n",
    "    # target accuracy parameters\n",
    "    target_acc = [0.90, 0.95, 0.99, 0.995, 1.0]\n",
    "    target_acc_index = 0\n",
    "    \n",
    "    # oscillation checking parameters\n",
    "    is_osc = False\n",
    "    osc_count = 0\n",
    "\n",
    "    # training parameters\n",
    "    num_epochs = 1000\n",
    "    max_grad_norm = 1.0\n",
    "\n",
    "    # weight decay parameter\n",
    "    weight_decay = enable_weight_decay\n",
    "    \n",
    "    return target_acc, target_acc_index, is_osc, osc_count, num_epochs, max_grad_norm, weight_decay\n",
    "\n",
    "''' \n",
    "Function that loads variables from saved .pth file. Checks if the file is valid.\n",
    "This function simply receives the checkpoint file and checks if it is valid.\n",
    "Inputs:\n",
    "    1) checkpoint_path: string of .pth file name \n",
    "    2) device: selected device to run training\n",
    "Returns:\n",
    "    1) pth_variables: dictionary that contains received .pth variables.\n",
    "    List of variables are saved in 'load_values' array.\n",
    "'''\n",
    "def load_pth_file(checkpoint_path, device, load_values):\n",
    "    pth_variables = {}\n",
    "    # Load .pth file\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)\n",
    "        for values in load_values:\n",
    "            pth_variables[values] = checkpoint.get(values, 0)\n",
    "        return pth_variables\n",
    "    except FileNotFoundError:\n",
    "        print(f\"No checkpoint found at {checkpoint_path}. Starting from scratch.\")\n",
    "        raise\n",
    "    except KeyError as e:\n",
    "        missing_keys = {\n",
    "            'cnn_state_dict': \"CNN state dictionary\",\n",
    "            'optimizer_state_dict': \"optimizer state dictionary\",\n",
    "            'epoch': \"epoch\",\n",
    "            'reached_acc': \"recorded accuracy of current epoch\",\n",
    "            'osc_count': \"oscillation count\"\n",
    "        }\n",
    "        key = e.args[0]\n",
    "        if key in missing_keys:\n",
    "            if key in ['cnn_state_dict', 'optimizer_state_dict', 'epoch', 'reached_acc']:\n",
    "                print(f\"Critical error: Missing \\\"{key}\\\" that stores \\\"{missing_keys[key]}\\\". Checkpoint file is invalid.\")\n",
    "                raise\n",
    "            else:\n",
    "                print(f\"Warning: Missing \\\"{key}\\\" that stores \\\"{missing_keys[key]}\\\". Default values will be used.\")\n",
    "                if key == 'osc_count':\n",
    "                    osc_count = 0\n",
    "\n",
    "        print(f\"Unknown parameter \\\"{key}\\\" in checkpoint. Recheck file and try again.\")\n",
    "        raise\n",
    "\n",
    "''' \n",
    "Function that updates learning rate accordingly from current reached target accuracy.\n",
    "This function allows changing values in the target_acc array without additional components.\n",
    "Inputs:\n",
    "    1) target_acc: array with target accuracies which reached updates the learning rate\n",
    "    2) target_acc_index: index value of previous target accuracy value\n",
    "    3) learning_rate: learning rate from CNN\n",
    "    4) reached_acc: previously reached accuracy\n",
    "    5) force_update: boolean that specifies if the function is called intentionally.\n",
    "    If set to True, print results\n",
    "Returns:\n",
    "    1) learning_rate: new updated learning rate\n",
    "    2) target_acc_index: updated index value of new target accuracy value\n",
    "'''\n",
    "def update_learning_rate(target_acc, target_acc_index, learning_rate, reached_acc, force_update):\n",
    "    temp = learning_rate\n",
    "    updated = False\n",
    "    while target_acc_index < len(target_acc) - 1 and target_acc[target_acc_index] < reached_acc:\n",
    "        updated = True\n",
    "        target_acc_index += 1\n",
    "        learning_rate /= divide_lr * (target_acc_index)\n",
    "    if force_update:\n",
    "        print(f\"\\tUpdated target accuracy: {target_acc[target_acc_index]}\")\n",
    "        print(f\"\\tUpdated learning rate: {learning_rate}\")\n",
    "    elif updated:\n",
    "        print(f\"Reached target accuracy {target_acc[target_acc_index-1]}\")\n",
    "        print(f\"New target accuracy {target_acc[target_acc_index]}\")\n",
    "        print(f\"Updating learning rate FROM: {temp}, TO: {learning_rate}\")\n",
    "    return learning_rate, target_acc_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnns = []\n",
    "dataloaders = builder.dataloaders \n",
    "timestamp   = datetime.datetime.now().strftime('%Y%m%d_%H%M')\n",
    "\n",
    "print(f\"Installed CUDA version: {torch.version.cuda}\\n\")\n",
    "\n",
    "# Create CNN per dataset\n",
    "for i in range(7,-1,-1):\n",
    "    print(f\"Starting training for \\\"cnn_{i}\\\"...\")\n",
    "\n",
    "    # initialize CNN\n",
    "    cnn, criterion, learning_rate, optimizer = cnn_init()\n",
    "    # Save default learning rate\n",
    "    default_lr = learning_rate\n",
    "    # initialize parameters\n",
    "    target_acc, target_acc_index, is_osc, osc_count, num_epochs, max_grad_norm, weight_decay = param_init()\n",
    "\n",
    "    if weight_decay:\n",
    "        print(\"\\tWeight decay enabled. Learning rate update and oscillation detection enabled.\")\n",
    "    else:\n",
    "        print(\"\\tWeight decay disabled. Learning rate update and oscillation detection disabled.\")\n",
    "\n",
    "    # Append CNN to cnns array\n",
    "    cnns.append(cnn)\n",
    "\n",
    "    # Create checkpoint to save progress\n",
    "    checkpoint_path = f\"resnet101_checkpoint_{i}.pth\"\n",
    "    \n",
    "    # Set device to cuda if available\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"\\tGPU found, running training on GPU...\")\n",
    "        device = torch.device(\"cuda\")\n",
    "        cnn = cnn.to(device)\n",
    "    else:\n",
    "        print(\"\\tNo GPU found, running training on CPU...\")\n",
    "        print(\"\\tRecheck CUDA version and if your GPU supports it.\")\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    # Attempt to load saved .pth file\n",
    "    # start_epoch = LOCAL variable that specifies starting epoch number\n",
    "    # prev_acc = LOCAL variable that specifies accuracy achieved by loaded .pth file\n",
    "    # osc_count = variable used to check oscillation, same name as dictionary key in .pth file\n",
    "    load_values = ['cnn_state_dict', 'optimizer_state_dict', 'epoch', 'reached_acc', 'osc_count']\n",
    "    try:\n",
    "        pth_vars = load_pth_file(checkpoint_path, device, load_values)\n",
    "    except FileNotFoundError:\n",
    "        start_epoch = 0\n",
    "        prev_acc = 0\n",
    "        osc_count = 0\n",
    "    else:\n",
    "        # Manually create variables per value\n",
    "        cnn.load_state_dict(pth_vars['cnn_state_dict'])\n",
    "        optimizer.load_state_dict(pth_vars['optimizer_state_dict'])\n",
    "        start_epoch = pth_vars['epoch']\n",
    "        prev_acc = pth_vars['reached_acc']\n",
    "        osc_count = pth_vars['osc_count']\n",
    "    \n",
    "        print(f\"Checkpoint loaded. Resuming from epoch {start_epoch}.\")\n",
    "        print(f\"\\tPrevious reached accuracy: {prev_acc}.\")\n",
    "\n",
    "    # Skip training if target accuracy is reached\n",
    "    if prev_acc == 1:\n",
    "        print(f\"\\tSkipping training: accuracy of 1 already achieved.\\n\")\n",
    "        continue\n",
    "    \n",
    "    if weight_decay:\n",
    "        # Adjust learning rate\n",
    "        learning_rate, target_acc_index = update_learning_rate(target_acc, target_acc_index, learning_rate, prev_acc, force_update=True)\n",
    "        optimizer = optim.Adam(cnn.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Start training\n",
    "    for e in range(start_epoch, num_epochs):\n",
    "        correct = 0\n",
    "        cnn.train()\n",
    "\n",
    "        for inputs, labels in dataloaders[i]:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # Add dimensions for channels and width\n",
    "            inputs = inputs.unsqueeze(1).unsqueeze(-1)\n",
    "            optimizer.zero_grad()\n",
    "            output = cnn(inputs)\n",
    "            # Check for NaN in outputs\n",
    "            if torch.isnan(output).any():\n",
    "                print(\"NaN detected in cnn outputs.\")\n",
    "                break\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "            # Check for NaN in loss\n",
    "            if torch.isnan(loss):\n",
    "                print(\"NaN detected in loss. Stopping training.\")\n",
    "                break\n",
    "            # print(f\"Loss: {loss.item()}\")\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(cnn.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            correct += (predicted == labels).sum()\n",
    "        \n",
    "        accuracy = correct / 256\n",
    "\n",
    "        # Change rate of update for printing accuracy accordingly\n",
    "        if (e + 1) % 1 == 0:\n",
    "            print(f'TRAINING: cnn[{i}], Epoch {e+1}, Loss: {loss.item()}')\n",
    "            print(f'TRAINING: cnn[{i}], Epoch {e+1}, Accuracy: {accuracy}')\n",
    "\n",
    "        if weight_decay:\n",
    "            # Check oscillation and update learning rate\n",
    "            # 1) is_osc == False: not in reset phase\n",
    "            # 1-a) check difference between accuracy and prev_acc, update osc_count\n",
    "            # 1-b) IF updated osc_count > 7, means oscillating, set learning rate to default value\n",
    "            # 1-c) ELSE not oscillating, check if target learning rate is reached\n",
    "            # 2) is_osc == True: in reset phase\n",
    "            # 2-a) check if osc_count == 0\n",
    "            # 2-b) IF osc_count == 0, reset phase is complete, re-update learning rate\n",
    "            if not is_osc:\n",
    "                if abs(accuracy - prev_acc) < 0.001:\n",
    "                    osc_count += 1\n",
    "                else:\n",
    "                    osc_count = 0\n",
    "                if osc_count > 7:\n",
    "                    print(f\"\\tOSCILLATION DETECTED. Resetting learning rate to {default_lr}...\")\n",
    "                    is_osc = True\n",
    "                    learning_rate = default_lr\n",
    "                    osc_count = 3\n",
    "                else:\n",
    "                    # Adjust learning rate\n",
    "                    learning_rate, target_acc_index = update_learning_rate(target_acc, target_acc_index, learning_rate, accuracy, force_update=False)\n",
    "                    optimizer = optim.Adam(cnn.parameters(), lr=learning_rate)\n",
    "            else:\n",
    "                if osc_count == 0:\n",
    "                    # Adjust learning rate\n",
    "                    print(\"\\tLearning rate reset completed, adjusting learning rate...\")\n",
    "                    learning_rate, target_acc_index = update_learning_rate(target_acc, 0, learning_rate, accuracy, force_update=True)\n",
    "                    optimizer = optim.Adam(cnn.parameters(), lr=learning_rate)\n",
    "                    is_osc = False\n",
    "                else:\n",
    "                    osc_count -= 1\n",
    "\n",
    "        # Save checkpoint\n",
    "        torch.save({\n",
    "            'cnn_state_dict': cnn.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'epoch': e + 1,\n",
    "            'reached_acc': accuracy,\n",
    "            'osc_count': osc_count\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved for epoch {e + 1}\")\n",
    "\n",
    "        prev_acc = accuracy\n",
    "\n",
    "        if accuracy == 1:\n",
    "            print(f\"Reached accuracy of 1. Stopping training for \\\"cnn_{i}\\\".\\n\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the CNNs\n",
    "\n",
    "Using the resulting CNNs saved in the .pth files, we test each CNNs using the provided power traces.\n",
    "\n",
    "Currently the testing data is a subset of the training data. In the future, if we can generate more data, we will be able to use separate datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation using testing data\n",
    "# CURRENTLY USING TRAINING DATA TO TEST DATA: use separate dataset in the future\n",
    "for i in range(7,-1,-1):\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)\n",
    "        cnn.load_state_dict(checkpoint['cnn_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        print(f\"Checkpoint loaded. Resuming from epoch {start_epoch}\")\n",
    "        reached_target_acc = checkpoint['reached_acc']\n",
    "        print(f\"Previous reached accuracy: {reached_target_acc}\")\n",
    "        if reached_target_acc != 1:\n",
    "            print(f\"cnn_{i}\\\" did not reach accuracy of 1, skipping evaluation.\\n\\n\")\n",
    "            continue\n",
    "    except FileNotFoundError:\n",
    "        print(f\"No checkpoint found for \\\"cnn_{i}\\\". Starting from scratch.\")\n",
    "\n",
    "    cnns[i].eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloaders[i]:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # Add dimensions for channels and width\n",
    "            inputs = inputs.unsqueeze(1).unsqueeze(-1)\n",
    "            outputs = cnns[i](inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Accuracy: {100 * correct / total:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
