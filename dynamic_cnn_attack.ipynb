{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN attack using Skywater non-linearized data\n",
    "\n",
    "The goal of this notebook is to correctly preprocess the given data as tensors, train and test a CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader.py\n",
    "# Necessary imports\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating dataloaders\n",
    "Used files from Kareem's GitHub repo. I may have made mistakes in sampling the data, so feel free to change anything that has been configured incorrectly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions - Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader.py\n",
    "# Returns list of files with given format \n",
    "def get_files(directory, format, digital_index=0):\n",
    "\n",
    "    format = re.compile(format)\n",
    "    files = os.listdir(directory)\n",
    "\n",
    "    #file_dict = {}\n",
    "    file_list = [] # fname, fpath, label\n",
    "\n",
    "    for fname in files:\n",
    "        if match := format.match(fname):\n",
    "            fpath = os.path.join(directory, fname)\n",
    "\n",
    "            dvalue = int(match.groups()[digital_index])\n",
    "            \n",
    "            file_list.append((fname, fpath, dvalue))\n",
    "\n",
    "            #if dvalue in file_dict:\n",
    "            #    file_dict[dvalue].append(fpath)\n",
    "            #else:\n",
    "            #    file_dict[dvalue] = [fpath]\n",
    "\n",
    "    return file_list #file_dict, file_path\n",
    "\n",
    "# Creates dataset with given traces\n",
    "class TraceDataset(Dataset):\n",
    "    cached_traces = {}\n",
    "    trace_list    = []\n",
    "\n",
    "    def __init__(self, file_list, cache=True):\n",
    "        self.file_list = file_list\n",
    "        self.cache     = cache\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        fname, fpath, label = self.file_list[index]\n",
    "        label = self.process_label(label)\n",
    "\n",
    "        if self.cache and fname in self.cached_traces:\n",
    "            return self.cached_traces[fname], label\n",
    "        else:\n",
    "            return self.load_trace(fname, fpath), label\n",
    "\n",
    "    def get_info(self, index):\n",
    "        return self.file_list[index]\n",
    "\n",
    "    def load_trace(self, fname, fpath):\n",
    "        with open(fpath, 'r') as file:\n",
    "            header = file.readline()\n",
    "            #time_arr = []\n",
    "            valu_arr = []\n",
    "            # Fixed error of float32 incorrectly translating values\n",
    "            for line in file.readlines():\n",
    "                time, value = line.strip().split()\n",
    "                try:\n",
    "                    match = re.search(r\"(?<=e-)\\d+\", value)\n",
    "                    if match:\n",
    "                        if value[0] == \"-\":\n",
    "                            strip_val = value[0:11]\n",
    "                        else:\n",
    "                            strip_val = value[0:10]\n",
    "                    float_val = np.float64(strip_val)\n",
    "                    rounded_val = round(float_val, 6)\n",
    "                    valu_arr.append(np.float32(rounded_val))\n",
    "\n",
    "                except ValueError as e:\n",
    "                    print(f\"Error parsing value '{value}': {e}\")\n",
    "\n",
    "        trace = np.array(valu_arr, dtype=np.float32)\n",
    "\n",
    "        if self.cache: \n",
    "            self.cached_traces[fname] = trace\n",
    "            self.trace_list.append(trace)\n",
    "\n",
    "        return trace\n",
    "    \n",
    "    def process_label(self, label): return label\n",
    "\n",
    "    def cache_all(self):\n",
    "        assert self.cache == True\n",
    "\n",
    "        print(\"Caching all traces\")\n",
    "        for fname, fpath, label in self.file_list:\n",
    "            self.load_trace(fname, fpath)\n",
    "        print(\"DONE Caching all traces\")\n",
    "\n",
    "class TraceDatasetBW(TraceDataset):\n",
    "    def __init__(self, file_list, bit_select, cache=True):\n",
    "        self.bit_mask = 1 << bit_select\n",
    "        super().__init__(file_list, cache=cache)\n",
    "\n",
    "    def process_label(self, label):\n",
    "        return 1 if label & self.bit_mask else 0\n",
    "\n",
    "class TraceDatasetBuilder:\n",
    "    def __init__(self, adc_bitwidth=8, cache=True):\n",
    "        self.file_list        = []\n",
    "        self.cache = cache\n",
    "        self.adc_bits = adc_bitwidth\n",
    "\n",
    "        self.dataset = None\n",
    "        self.dataloader = None\n",
    "        self.datasets = []\n",
    "        self.dataloaders = []\n",
    "\n",
    "    def add_files(self, directory, format, label_group):\n",
    "        ''' Builds list of powertrace files\n",
    "        Inputs:\n",
    "            directory   : folder to search for files\n",
    "            format      : regular expression to match filenames\n",
    "            label_index : group index for digital output label corresponding to trace\n",
    "        Outputs:\n",
    "            list        : [(file_name, file_path, label) ... ]\n",
    "        '''\n",
    "        format = re.compile(format)\n",
    "        fnames = os.listdir(directory)\n",
    "\n",
    "        for fname in fnames:\n",
    "            if match := format.match(fname):\n",
    "                fpath = os.path.join(directory, fname)\n",
    "                dvalue = int(match.groups()[label_group])\n",
    "\n",
    "                self.file_list.append((fname, fpath, dvalue))\n",
    "\n",
    "    def build(self):\n",
    "        self.dataset = TraceDataset(self.file_list, cache=self.cache)\n",
    "        for b in range(self.adc_bits):\n",
    "            self.datasets.append(TraceDatasetBW(self.file_list, b, cache=self.cache))\n",
    "\n",
    "        if self.cache:\n",
    "            self.dataset.cache_all()\n",
    "\n",
    "    def build_dataloaders(self, **kwargs): # batch_size=256, shuffle=True\n",
    "        self.dataloader = DataLoader(self.dataset, **kwargs)\n",
    "        self.dataloaders = [DataLoader(dataset, **kwargs) for dataset in self.datasets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating dataloaders\n",
    "This section:\n",
    "1) Creates dataloaders for both the training and testing datasets.\n",
    "2) Validates and checks the contents of each dataset.\n",
    "3) Common variables, such as \"input_size\" are instantiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for dataloaders\n",
    "adc_bits = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching all traces\n",
      "DONE Caching all traces\n",
      "\n",
      "Total number of datasets in dataloaders: 8\n",
      "Input size: 3001\n",
      "Number of labels: 256\n",
      "\n",
      "Validation for dataloader 0 complete.\n",
      "Validation for dataloader 1 complete.\n",
      "Validation for dataloader 2 complete.\n",
      "Validation for dataloader 3 complete.\n",
      "Validation for dataloader 4 complete.\n",
      "Validation for dataloader 5 complete.\n",
      "Validation for dataloader 6 complete.\n",
      "Validation for dataloader 7 complete.\n",
      "\n",
      "All dataloaders are valid. Proceeding to training...\n"
     ]
    }
   ],
   "source": [
    "# Create dataloaders\n",
    "pwd = os.getcwd()\n",
    "# print(pwd)\n",
    "# proj_dir = os.path.dirname(os.path.dirname(pwd))\n",
    "# print(proj_dir)\n",
    "# data_dir = os.path.join(pwd, 'analog', 'outfiles')\n",
    "\n",
    "builder  = TraceDatasetBuilder(adc_bitwidth=adc_bits, cache=True)\n",
    "builder.add_files(os.path.join(pwd, 'sky_Dec_18_2151'), \"sky_d(\\\\d+)_.*\\\\.txt\", 0)\n",
    "builder.build()\n",
    "builder.build_dataloaders(batch_size=256, shuffle=True)\n",
    "\n",
    "# Initialize dataloaders\n",
    "dataloaders = builder.dataloaders\n",
    "\n",
    "try:\n",
    "    print(f\"\\nTotal number of datasets in dataloaders: {len(dataloaders)}\")\n",
    "except ModuleNotFoundError:\n",
    "    print(\"\\tERROR: Dataloader not found. Recheck your implementation and try again.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(\"\\tERROR: Dataloader invalid. Recheck your implementation and try again.\")\n",
    "    raise\n",
    "\n",
    "if not dataloaders or len(dataloaders) != adc_bits:\n",
    "    print(f\"\\tERROR: Dataloader size is incorrect: {len(dataloaders) if dataloaders else 0}. It should be: {adc_bits}.\")\n",
    "    print(\"\\tRecheck your implementation and try again.\")\n",
    "    raise ValueError(\"Invalid dataloader size\")\n",
    "\n",
    "# Extract input_size and validate initial dataloader\n",
    "input_size, label_num = None, None\n",
    "for input, labels in dataloaders[0]:\n",
    "    input_size = input.size(1)\n",
    "    label_num = len(labels)\n",
    "    print(f\"Input size: {input_size}\")\n",
    "    print(f\"Number of labels: {label_num}\\n\")\n",
    "\n",
    "    if input.size(0) != label_num:\n",
    "        print(\"\\tERROR: Number of traces per input is different from number of labels.\")\n",
    "        print(f\"\\tNumber of traces per input: {input.size(0)}\")\n",
    "        print(f\"\\tNumber of labels: {label_num}\\n\")\n",
    "        raise ValueError(\"Mismatch between input size and number of labels\")\n",
    "\n",
    "# Validate all dataloaders\n",
    "for idx, dataloader in enumerate(dataloaders):\n",
    "    for input, labels in dataloader:\n",
    "        if input.size(0) != label_num:\n",
    "            print(f\"\\tERROR: Number of traces per input is incorrect in dataloader {idx}.\")\n",
    "            print(f\"\\tExpected: {label_num}, but got: {input.size(0)}.\")\n",
    "            raise ValueError(\"Invalid number of traces per input\")\n",
    "\n",
    "        if input.size(1) != input_size:\n",
    "            print(f\"\\tERROR: Size of traces per input is incorrect in dataloader {idx}.\")\n",
    "            print(f\"\\tExpected: {input_size}, but got: {input.size(1)}.\")\n",
    "            raise ValueError(\"Invalid trace size\")\n",
    "\n",
    "        if len(labels) != label_num:\n",
    "            print(f\"\\tERROR: Number of labels is incorrect in dataloader {idx}.\")\n",
    "            print(f\"\\tExpected: {label_num}, but got: {len(labels)}.\")\n",
    "            raise ValueError(\"Invalid label count\")\n",
    "\n",
    "    print(f\"Validation for dataloader {idx} complete.\")\n",
    "\n",
    "print(\"\\nAll dataloaders are valid. Proceeding to training...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for CNN\n",
    "Section where the initial imports and variables for our native CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN code\n",
    "# Necessary imports\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing CUDA version\n",
    "Section where we desginate the most updated CUDA version for current GPU.\n",
    "\n",
    "Implementation in progress, but is a lot of unnecessary work; will do if CPU training speeds are unformidable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training CNNs - Native CNN\n",
    "Using a native CNN, we train each CNN until all of them reaches an accuracy of 1.0.\n",
    "\n",
    "I'm not sure if aiming for an accuracy of 1.0 is beneficial, as it is just overfitting the model to the training data. A more realistic value may be 0.99, but will set it to 1.0 for the current simulated environment.\n",
    "\n",
    "The training function automatically reduces the learning rate used in Adam based on target accuracy. Currently testing different values and decrease rates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "1) def_lr: Default learning rate. Change this value to test different learning rate values.\n",
    "2) max_epoch: Limit of maximum training epochs; training ends if it reaches this amount of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main hyperparameters\n",
    "def_lr = 1e-4\n",
    "max_epoch = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for optimal CNN parameters\n",
    "\n",
    "Paramerize components of CNN and find best overall hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import hashlib\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "from copy import copy\n",
    "\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Hyperparameter format\n",
    "\n",
    "#### 1. Convolutional Layer (Conv1d)\n",
    "##### Format:\n",
    "```\n",
    "conv_[# of conv layer]_[# of out_channels]_[kernel_size #]_[stride #]_[padding #]_[dilation #]\n",
    "```\n",
    "##### Example:\n",
    "```python\n",
    "self.conv1 = nn.Conv1d(in_channels=1, out_channels=8, kernel_size=5, stride=1, padding=2, dilation=1)\n",
    "# Naming: conv_1_1_8_5_1_2_1\n",
    "```\n",
    "> **Note:** Padding mode = `'zeros'`\n",
    "\n",
    "#### 2. Max Pooling Layer (MaxPool1d)\n",
    "##### Format:\n",
    "```\n",
    "maxpool_[# of pool layer]_[kernel_size #]_[stride #]_[padding #]_[dilation #]\n",
    "```\n",
    "##### Example:\n",
    "```python\n",
    "self.pool1 = nn.MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1)\n",
    "# Naming: maxpool_1_3_3_0_1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Fully Connected Layer (Linear)\n",
    "##### Format:\n",
    "```\n",
    "linear_[# of fc layer]_[in_features #]_[out_features #]\n",
    "```\n",
    "##### Example:\n",
    "```python\n",
    "self.fc1 = nn.Linear(500, 500)\n",
    "# Naming: linear_1_500_500\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter update testing area\n",
    "# Change values within this section to attempt training on CNN\n",
    "# IN FUTURE: Update functions to use different datasets, will test ONLY on non-linerized data\n",
    "\n",
    "# default for both conv and maxpool: padding=0, dilation=1\n",
    "# conv_[# of conv layer]_[# of out_channels]_[kernel_size #]_[stride #]_[padding #]_[dilation #]\n",
    "# maxpool_[# of pool layer]_[kernel_size #]_[stride #]_[padding #]_[dilation #]\n",
    "# linear_[# of fc layer]_[in_features #]_[out_features #]\n",
    "cnn_format = {\n",
    "    \"conv_params\": [\"conv_1_8_5_1_2_1\",\n",
    "                    \"conv_2_8_5_1_2_1\",\n",
    "                    \"conv_3_8_5_1_2_1\",\n",
    "                    \"conv_4_8_5_1_2_1\",\n",
    "                    \"conv_5_8_5_1_2_1\",\n",
    "                    \"conv_6_8_5_1_2_1\",\n",
    "                    \"conv_1_16_5_1_2_1\",\n",
    "                    \"conv_2_16_5_1_2_1\",\n",
    "                    \"conv_3_16_5_1_2_1\",\n",
    "                    \"conv_4_16_5_1_2_1\",\n",
    "                    \"conv_5_16_5_1_2_1\",\n",
    "                    \"conv_6_16_5_1_2_1\",\n",
    "                    ],\n",
    "    \"maxpool_params\" : [\"maxpool_3_3_0_1\",\n",
    "                        \"maxpool_5_5_0_1\",\n",
    "                    ],\n",
    "    \"linear_params\" : [\"linear_1_500_500\",\n",
    "                       \"linear_2_500_500\",\n",
    "                       \"linear_3_500_500\",\n",
    "                       \"linear_4_500_500\",\n",
    "                    ],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Parsing area, constructs CNNs using hyperparameters\\ncnn_pattern = r\"conv_(\\\\d+)_(\\\\d+)_(\\\\d+)_(\\\\d+)_(\\\\d+)\"\\npool_pattern = r\"maxpool_(\\\\d+)_(\\\\d+)\"\\nlinear_pattern = r\"linear_(\\\\d+)_(\\\\d+)_(\\\\d+)\"\\n\\ndef extract_conv_params(conv_string):\\n    match = re.match(cnn_pattern, conv_string)\\n    if match:\\n        return {\\n            \"conv_layer_num\": int(match.group(1)),\\n            \"out_channels\": int(match.group(2)),\\n            \"kernel_size\": int(match.group(3)),\\n            \"stride\": int(match.group(4)),\\n            \"padding\": int(match.group(5)),\\n        }\\n    return None\\n\\ndef extract_pool_params(pool_string):\\n    match = re.match(pool_pattern, pool_string)\\n    if match:\\n        return {\\n            \"kernel_size\": int(match.group(2)),\\n            \"stride\": int(match.group(3))\\n        }\\n    return None\\n\\ndef extract_linear_params(linear_string):\\n    match = re.match(linear_pattern, linear_string)\\n    if match:\\n        return {\\n            \"linear_layer_num\": int(match.group(1)),\\n            \"in_features\": int(match.group(2)),\\n            \"out_features\": int(match.group(3))\\n        }\\n    return None\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Parsing area, constructs CNNs using hyperparameters\n",
    "cnn_pattern = r\"conv_(\\d+)_(\\d+)_(\\d+)_(\\d+)_(\\d+)\"\n",
    "pool_pattern = r\"maxpool_(\\d+)_(\\d+)\"\n",
    "linear_pattern = r\"linear_(\\d+)_(\\d+)_(\\d+)\"\n",
    "\n",
    "def extract_conv_params(conv_string):\n",
    "    match = re.match(cnn_pattern, conv_string)\n",
    "    if match:\n",
    "        return {\n",
    "            \"conv_layer_num\": int(match.group(1)),\n",
    "            \"out_channels\": int(match.group(2)),\n",
    "            \"kernel_size\": int(match.group(3)),\n",
    "            \"stride\": int(match.group(4)),\n",
    "            \"padding\": int(match.group(5)),\n",
    "        }\n",
    "    return None\n",
    "\n",
    "def extract_pool_params(pool_string):\n",
    "    match = re.match(pool_pattern, pool_string)\n",
    "    if match:\n",
    "        return {\n",
    "            \"kernel_size\": int(match.group(2)),\n",
    "            \"stride\": int(match.group(3))\n",
    "        }\n",
    "    return None\n",
    "\n",
    "def extract_linear_params(linear_string):\n",
    "    match = re.match(linear_pattern, linear_string)\n",
    "    if match:\n",
    "        return {\n",
    "            \"linear_layer_num\": int(match.group(1)),\n",
    "            \"in_features\": int(match.group(2)),\n",
    "            \"out_features\": int(match.group(3))\n",
    "        }\n",
    "    return None\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicCNN(nn.Module):\n",
    "    def __init__(self, conv_params, maxpool_params, linear_params, input_length=3001):\n",
    "        super(DynamicCNN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        in_channels = 1  # Initial input channel\n",
    "        final_size = input_length\n",
    "\n",
    "        # Parse convolutional layer parameters\n",
    "        try:\n",
    "            conv_layer_num, out_channels, conv_kernel_size, conv_stride, conv_padding, conv_dilation = map(int, conv_params.split(\"_\")[1:])\n",
    "        except ValueError:\n",
    "            raise ValueError(\"Error: conv_params format is incorrect.\")\n",
    "\n",
    "        # Parse max-pooling layer parameters\n",
    "        try:\n",
    "            pool_kernel_size, pool_stride, pool_padding, pool_dilation = map(int, maxpool_params.split(\"_\")[1:])\n",
    "        except ValueError:\n",
    "            raise ValueError(\"Error: maxpool_params format is incorrect.\")\n",
    "\n",
    "        # Convolutional layers\n",
    "        for _ in range(conv_layer_num):\n",
    "            self.layers.append(nn.Conv1d(in_channels, out_channels, conv_kernel_size, conv_stride, conv_padding, conv_dilation))\n",
    "            final_size = (final_size + 2 * conv_padding - conv_dilation * (conv_kernel_size - 1) - 1) // conv_stride + 1\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.MaxPool1d(pool_kernel_size, pool_stride, pool_padding, pool_dilation))\n",
    "            final_size = (final_size + 2 * pool_padding - pool_dilation * (pool_kernel_size - 1) - 1) // pool_stride + 1\n",
    "            in_channels = out_channels  # Update input channels for the next layer\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        try:\n",
    "            fc_layer_num, in_features, out_features = map(int, linear_params.split(\"_\")[1:])\n",
    "        except ValueError:\n",
    "            print(\"Error: linear_params format is incorrect.\")\n",
    "            raise\n",
    "        # First FC has in features: flat_size * out_channels\n",
    "        self.fc_layers.append(nn.Linear(out_channels * final_size, out_features))\n",
    "        self.fc_layers.append(nn.ReLU())\n",
    "        for i in range(1,int(fc_layer_num)):\n",
    "            self.fc_layers.append(nn.Linear(in_features, out_features))\n",
    "            self.fc_layers.append(nn.ReLU())\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(out_features, 2)  # Binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        try:\n",
    "            x = x.unsqueeze(1)  # Add channel dimension\n",
    "\n",
    "            # Pass through convolutional layers\n",
    "            for layer in self.layers:\n",
    "                x = layer(x)\n",
    "\n",
    "            # Flatten dynamically\n",
    "            x = x.view(x.size(0), -1)  # Convert to (batch_size, flattened_features)\n",
    "\n",
    "            # Fully connected layers\n",
    "            for layer in self.fc_layers:\n",
    "                x = layer(x)\n",
    "\n",
    "            x = torch.softmax(self.output_layer(x), dim=1)\n",
    "            return x\n",
    "        except RuntimeError:\n",
    "            print(\"Invalid form, skipping now...\")\n",
    "            x = torch.tensor([1]).float()\n",
    "            x[x == 1] = float('nan')\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN model initialization\n",
    "Function that initializes the CNN architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_allowed_epoch = {\n",
    "        \"7\": 100,\n",
    "        \"6\": 1600,\n",
    "        \"5\": 1600,\n",
    "        \"4\": 1600,\n",
    "        \"3\": 1600,\n",
    "        \"2\": 1600,\n",
    "        \"1\": 1600,\n",
    "        \"0\": 1600\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Function that initializes the CNN and its core components.\n",
    "Inputs:\n",
    "    1) cnn_num: cnn bit number\n",
    "Returns:\n",
    "    1) cnn: model used(ResNet18)\n",
    "    2) criterion: loss function, current default=CrossEntropyLoss\n",
    "    3) learning_rate: learning rate, current default=def_lr\n",
    "    4) optimizer: optimizer, current default=Adam\n",
    "'''\n",
    "def dynamic_cnn_init(cnn_param_dict, learning_rate):\n",
    "    tot_cnns = 0\n",
    "    test_cnns = []\n",
    "    test_optims = []\n",
    "    cnn_strings = []\n",
    "    for conv_param in cnn_param_dict[\"conv_params\"]:\n",
    "        for maxpool_param in cnn_param_dict[\"maxpool_params\"]:\n",
    "            for linear_param in cnn_param_dict[\"linear_params\"]:\n",
    "                new_cnn = DynamicCNN(conv_param, maxpool_param, linear_param)\n",
    "                # Optimizer: not specified in paper, using Adam\n",
    "                new_optim = optim.Adam(new_cnn.parameters(), lr=learning_rate)\n",
    "                test_cnns.append(new_cnn)\n",
    "                test_optims.append(new_optim)\n",
    "                tot_cnns += 1\n",
    "                cnn_strings.append(f\"{conv_param}_{maxpool_param}_{linear_param}\")\n",
    "    # Loss function: not specified in paper, using Cross Entropy Loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Learning rate: default set to def_lr, adjust accordingly\n",
    "    learning_rate = def_lr\n",
    "    return test_cnns, test_optims, criterion, learning_rate, tot_cnns, cnn_strings\n",
    "\n",
    "''' \n",
    "Function that initializes parameters used in training.\n",
    "Inputs:\n",
    "    None\n",
    "Returns:\n",
    "    TRAINING PARAMETERS\n",
    "    1) num_epochs: number of maximum epochs per training\n",
    "    2) max_grad_norm: gradient clipping threshold\n",
    "'''\n",
    "def param_init():\n",
    "    # training parameters\n",
    "    max_grad_norm = 1.0\n",
    "\n",
    "    return max_grad_norm\n",
    "\n",
    "''' \n",
    "Function that loads variables from saved .pth file. Checks if the file is valid.\n",
    "This function simply receives the checkpoint file and checks if it is valid.\n",
    "Inputs:\n",
    "    1) checkpoint_path: string of .pth file name \n",
    "    2) device: selected device to run training\n",
    "Returns:\n",
    "    1) pth_variables: dictionary that contains received .pth variables.\n",
    "    List of variables are saved in 'load_values' array.\n",
    "'''\n",
    "def load_pth_file(checkpoint_path, device, load_values):\n",
    "    pth_variables = {}\n",
    "    # Load .pth file\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)\n",
    "        for values in load_values:\n",
    "            pth_variables[values] = checkpoint.get(values, 0)\n",
    "        return pth_variables\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\tNo checkpoint found at {checkpoint_path}. Starting from scratch.\")\n",
    "        raise\n",
    "    except KeyError as e:\n",
    "        missing_keys = {\n",
    "            'cnn_state_dict': \"CNN state dictionary\",\n",
    "            'optimizer_state_dict': \"optimizer state dictionary\",\n",
    "            'epoch': \"epoch\",\n",
    "            'reached_acc': \"recorded accuracy of current epoch\"\n",
    "        }\n",
    "        key = e.args[0]\n",
    "        if key in missing_keys:\n",
    "            if key in ['cnn_state_dict', 'optimizer_state_dict', 'reached_acc']:\n",
    "                print(f\"\\t\\tCritical error: Missing \\\"{key}\\\" that stores \\\"{missing_keys[key]}\\\". Checkpoint file is invalid.\")\n",
    "                raise\n",
    "            else:\n",
    "                print(f\"\\t\\tWarning: Missing \\\"{key}\\\" that stores \\\"{missing_keys[key]}\\\". Default values will be used.\")\n",
    "                if key == 'epoch':\n",
    "                    epoch = 0\n",
    "\n",
    "        print(f\"\\tUnknown parameter \\\"{key}\\\" in checkpoint. Recheck file and try again.\")\n",
    "        raise\n",
    "\n",
    "def train_cnns(cnn_num, test_num, test_cnn, test_optim, cnn_string, test_criterion, max_epoch):\n",
    "    dataloaders = builder.dataloaders \n",
    "    print(f\"\\tStarting training for \\\"cnn_{cnn_num}, {cnn_string}\\\"...\")\n",
    "\n",
    "    # initialize CNN\n",
    "    cnn, criterion, optimizer = test_cnn, test_criterion, test_optim\n",
    "    # initialize parameters\n",
    "    max_grad_norm = param_init()\n",
    "\n",
    "    # Create checkpoint to save progress\n",
    "    checkpoint_path = f\"cp_{cnn_num}_{cnn_string}.pth\"    \n",
    "    \n",
    "    # Set device to cuda if available\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        cnn = cnn.to(device)\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        cnn = cnn.to(device)\n",
    "\n",
    "    max_acc = 0\n",
    "    max_acc_epoch = 0\n",
    "\n",
    "    # Attempt to load saved .pth file\n",
    "    # start_epoch = LOCAL variable that specifies starting epoch number\n",
    "    # prev_acc = LOCAL variable that specifies accuracy achieved by loaded .pth file\n",
    "    load_values = ['cnn_state_dict', 'optimizer_state_dict', 'epoch', 'reached_acc']\n",
    "    try:\n",
    "        pth_vars = load_pth_file(checkpoint_path, device, load_values)\n",
    "    except FileNotFoundError:\n",
    "        start_epoch = 0\n",
    "        prev_acc = 0\n",
    "    else:\n",
    "        # Manually create variables per value\n",
    "        cnn.load_state_dict(pth_vars['cnn_state_dict'])\n",
    "        optimizer.load_state_dict(pth_vars['optimizer_state_dict'])\n",
    "        start_epoch = pth_vars['epoch']\n",
    "        prev_acc = pth_vars['reached_acc']\n",
    "        max_acc = prev_acc\n",
    "    \n",
    "        print(f\"\\tCheckpoint loaded. Resuming from epoch {start_epoch}.\")\n",
    "        print(f\"\\t\\tPrevious reached accuracy: {prev_acc}.\")\n",
    "        # Skip training if target accuracy is reached\n",
    "        if prev_acc == 1:\n",
    "            print(f\"\\t\\tSkipping training: accuracy of 1 already achieved.\\n\")\n",
    "            return start_epoch, prev_acc\n",
    "        # Skip training if maximum epoch is met\n",
    "        if start_epoch >= max_epoch:\n",
    "            print(f\"\\t\\tSkipping training: maximum allowed epoch of {max_epoch} already exhausted.\\n\")\n",
    "            return start_epoch, prev_acc\n",
    "\n",
    "    # Start training\n",
    "    for e in range(start_epoch, max_epoch):\n",
    "        try:\n",
    "            correct = 0\n",
    "            cnn.train()\n",
    "\n",
    "            for inputs, labels in dataloaders[cnn_num]:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                # Add dimensions for channels and width\n",
    "                # inputs = inputs.unsqueeze(1).unsqueeze(-1)\n",
    "                optimizer.zero_grad()\n",
    "                output = cnn(inputs)\n",
    "                # Check for NaN in outputs\n",
    "                if torch.isnan(output).any():\n",
    "                    print(\"\\tNaN detected in cnn outputs.\\n\")\n",
    "                    return 0, 0\n",
    "\n",
    "                loss = criterion(output, labels)\n",
    "                # Check for NaN in loss\n",
    "                if torch.isnan(loss):\n",
    "                    print(\"\\tNaN detected in loss. Stopping training.\\n\")\n",
    "                    return 0, 0\n",
    "                # print(f\"Loss: {loss.item()}\")\n",
    "                loss.backward()\n",
    "\n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(cnn.parameters(), max_grad_norm)\n",
    "                optimizer.step()\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = correct / 256\n",
    "            if accuracy > max_acc:\n",
    "                max_acc = accuracy\n",
    "                max_acc_epoch = e + 1\n",
    "\n",
    "            # Change rate of update for printing accuracy accordingly\n",
    "            if (e + 1) % 100 == 0:\n",
    "                '''\n",
    "                print(f'\\tTRAINING: cnn[{cnn_num}], {cnn_string}], Epoch {e+1}, Loss: {loss.item()}')\n",
    "                print(f'\\tTRAINING: cnn[{cnn_num}], {cnn_string}], Epoch {e+1}, Accuracy: {accuracy}')\n",
    "                print(f'\\tMax accuracy: {max_acc}')\n",
    "                print(f'\\tMax accuracy epoch: {max_acc_epoch}')\n",
    "                '''\n",
    "\n",
    "                # Save checkpoint\n",
    "                torch.save({\n",
    "                    'cnn_state_dict': cnn.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'epoch': e + 1,\n",
    "                    'reached_acc': accuracy\n",
    "                }, checkpoint_path)\n",
    "                print(f\"\\tCheckpoint saved for epoch {e + 1}, reached accuracy: {max_acc}\")\n",
    "\n",
    "            prev_acc = accuracy\n",
    "\n",
    "            if accuracy == 1:\n",
    "                print(f\"\\tReached accuracy of 1. Stopping training for \\\"cnn_{cnn_num}_{test_num}\\\".\")\n",
    "                print(f\"\\tAchieved epoch: {max_acc_epoch}\\n\")\n",
    "                # Save checkpoint\n",
    "                torch.save({\n",
    "                    'cnn_state_dict': cnn.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'epoch': e + 1,\n",
    "                    'reached_acc': accuracy\n",
    "                }, checkpoint_path)\n",
    "                return e + 1, 1\n",
    "            if e + 1 == max_epoch:\n",
    "                print(f\"\\tMaximum allowed epoch of {max_epoch} reached, aborting training.\\n\")\n",
    "                # Save checkpoint\n",
    "                torch.save({\n",
    "                    'cnn_state_dict': cnn.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'epoch': e + 1,\n",
    "                    'reached_acc': accuracy\n",
    "                }, checkpoint_path)\n",
    "                return e + 1, accuracy\n",
    "        except KeyboardInterrupt:\n",
    "            # Save checkpoint\n",
    "            torch.save({\n",
    "                'cnn_state_dict': cnn.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'epoch': e + 1,\n",
    "                'reached_acc': accuracy\n",
    "            }, checkpoint_path)\n",
    "            print(f\"\\tTraining interrupted. Pausing training...\")\n",
    "            print(f\"\\tCheckpoint saved for last epoch {e + 1}\")\n",
    "            print(f'\\t\\tFinal accuracy: {accuracy}')\n",
    "            return e + 1, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed CUDA version: None\n",
      "\tNo GPU found, running training on CPU...\n",
      "\tRecheck CUDA version and if your GPU supports it.\n"
     ]
    }
   ],
   "source": [
    "# Basic CUDA checkup\n",
    "# Set device to cuda if available\n",
    "print(f\"Installed CUDA version: {torch.version.cuda}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\tGPU found, running training on GPU...\")\n",
    "else:\n",
    "    print(\"\\tNo GPU found, running training on CPU...\")\n",
    "    print(\"\\tRecheck CUDA version and if your GPU supports it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Calvin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle file combined_result not found. combined_result not loaded.\n",
      "Starting training for \"cnn_7, conv_1_8_5_1_2_1_maxpool_3_3_0_1_linear_1_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 100.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 100 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_1_8_5_1_2_1_maxpool_3_3_0_1_linear_2_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 29.\n",
      "\tPrevious reached accuracy: 1.0.\n",
      "\tSkipping training: accuracy of 1 already achieved.\n",
      "\n",
      "Starting training for \"cnn_7, conv_1_8_5_1_2_1_maxpool_3_3_0_1_linear_3_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 29.\n",
      "\tPrevious reached accuracy: 0.96484375.\n",
      "\tSkipping training: maximum allowed epoch of 29 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_1_8_5_1_2_1_maxpool_3_3_0_1_linear_4_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 29.\n",
      "\tPrevious reached accuracy: 0.88671875.\n",
      "\tSkipping training: maximum allowed epoch of 29 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_1_8_5_1_2_1_maxpool_5_5_0_1_linear_1_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 29.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 29 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_1_8_5_1_2_1_maxpool_5_5_0_1_linear_2_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 29.\n",
      "\tPrevious reached accuracy: 0.87109375.\n",
      "\tSkipping training: maximum allowed epoch of 29 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_1_8_5_1_2_1_maxpool_5_5_0_1_linear_3_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 29.\n",
      "\tPrevious reached accuracy: 0.98046875.\n",
      "\tSkipping training: maximum allowed epoch of 29 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_1_8_5_1_2_1_maxpool_5_5_0_1_linear_4_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 29.\n",
      "\tPrevious reached accuracy: 0.98828125.\n",
      "\tSkipping training: maximum allowed epoch of 29 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_2_8_5_1_2_1_maxpool_3_3_0_1_linear_1_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 29.\n",
      "\tPrevious reached accuracy: 0.8671875.\n",
      "\tSkipping training: maximum allowed epoch of 29 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_2_8_5_1_2_1_maxpool_3_3_0_1_linear_2_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 29.\n",
      "\tPrevious reached accuracy: 0.9453125.\n",
      "\tSkipping training: maximum allowed epoch of 29 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_2_8_5_1_2_1_maxpool_3_3_0_1_linear_3_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 29.\n",
      "\tPrevious reached accuracy: 0.94140625.\n",
      "\tSkipping training: maximum allowed epoch of 29 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_2_8_5_1_2_1_maxpool_3_3_0_1_linear_4_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 29.\n",
      "\tPrevious reached accuracy: 0.91796875.\n",
      "\tSkipping training: maximum allowed epoch of 29 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_2_8_5_1_2_1_maxpool_5_5_0_1_linear_1_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 29.\n",
      "\tPrevious reached accuracy: 0.9453125.\n",
      "\tSkipping training: maximum allowed epoch of 29 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_2_8_5_1_2_1_maxpool_5_5_0_1_linear_2_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 29.\n",
      "\tPrevious reached accuracy: 0.94921875.\n",
      "\tSkipping training: maximum allowed epoch of 29 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_2_8_5_1_2_1_maxpool_5_5_0_1_linear_3_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 29.\n",
      "\tPrevious reached accuracy: 0.9453125.\n",
      "\tSkipping training: maximum allowed epoch of 29 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_2_8_5_1_2_1_maxpool_5_5_0_1_linear_4_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 29.\n",
      "\tPrevious reached accuracy: 0.91015625.\n",
      "\tSkipping training: maximum allowed epoch of 29 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_3_8_5_1_2_1_maxpool_3_3_0_1_linear_1_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 29.\n",
      "\tPrevious reached accuracy: 0.88671875.\n",
      "\tSkipping training: maximum allowed epoch of 29 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_3_8_5_1_2_1_maxpool_3_3_0_1_linear_2_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 29.\n",
      "\tPrevious reached accuracy: 0.9453125.\n",
      "\tSkipping training: maximum allowed epoch of 29 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_3_8_5_1_2_1_maxpool_3_3_0_1_linear_3_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 29.\n",
      "\tPrevious reached accuracy: 0.93359375.\n",
      "\tSkipping training: maximum allowed epoch of 29 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_3_8_5_1_2_1_maxpool_3_3_0_1_linear_4_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 29.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 29 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_3_8_5_1_2_1_maxpool_5_5_0_1_linear_1_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 29.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 29 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_3_8_5_1_2_1_maxpool_5_5_0_1_linear_2_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 29.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 29 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_3_8_5_1_2_1_maxpool_5_5_0_1_linear_3_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 29.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 29 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_3_8_5_1_2_1_maxpool_5_5_0_1_linear_4_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 29.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 29 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_4_8_5_1_2_1_maxpool_3_3_0_1_linear_1_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 29.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 29 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_4_8_5_1_2_1_maxpool_3_3_0_1_linear_2_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 29.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 29 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_4_8_5_1_2_1_maxpool_3_3_0_1_linear_3_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 29.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 29 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_4_8_5_1_2_1_maxpool_3_3_0_1_linear_4_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 29.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 29 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_4_8_5_1_2_1_maxpool_5_5_0_1_linear_1_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 29.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 29 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_4_8_5_1_2_1_maxpool_5_5_0_1_linear_2_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 1.0.\n",
      "\tSkipping training: accuracy of 1 already achieved.\n",
      "\n",
      "Starting training for \"cnn_7, conv_4_8_5_1_2_1_maxpool_5_5_0_1_linear_3_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_4_8_5_1_2_1_maxpool_5_5_0_1_linear_4_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_5_8_5_1_2_1_maxpool_3_3_0_1_linear_1_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_5_8_5_1_2_1_maxpool_3_3_0_1_linear_2_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_5_8_5_1_2_1_maxpool_3_3_0_1_linear_3_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_5_8_5_1_2_1_maxpool_3_3_0_1_linear_4_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_5_8_5_1_2_1_maxpool_5_5_0_1_linear_1_500_500\"...\n",
      "No checkpoint found at cp_7_conv_5_8_5_1_2_1_maxpool_5_5_0_1_linear_1_500_500.pth. Starting from scratch.\n",
      "Invalid form, skipping now...\n",
      "NaN detected in cnn outputs.\n",
      "\n",
      "Starting training for \"cnn_7, conv_5_8_5_1_2_1_maxpool_5_5_0_1_linear_2_500_500\"...\n",
      "No checkpoint found at cp_7_conv_5_8_5_1_2_1_maxpool_5_5_0_1_linear_2_500_500.pth. Starting from scratch.\n",
      "Invalid form, skipping now...\n",
      "NaN detected in cnn outputs.\n",
      "\n",
      "Starting training for \"cnn_7, conv_5_8_5_1_2_1_maxpool_5_5_0_1_linear_3_500_500\"...\n",
      "No checkpoint found at cp_7_conv_5_8_5_1_2_1_maxpool_5_5_0_1_linear_3_500_500.pth. Starting from scratch.\n",
      "Invalid form, skipping now...\n",
      "NaN detected in cnn outputs.\n",
      "\n",
      "Starting training for \"cnn_7, conv_5_8_5_1_2_1_maxpool_5_5_0_1_linear_4_500_500\"...\n",
      "No checkpoint found at cp_7_conv_5_8_5_1_2_1_maxpool_5_5_0_1_linear_4_500_500.pth. Starting from scratch.\n",
      "Invalid form, skipping now...\n",
      "NaN detected in cnn outputs.\n",
      "\n",
      "Starting training for \"cnn_7, conv_6_8_5_1_2_1_maxpool_3_3_0_1_linear_1_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_6_8_5_1_2_1_maxpool_3_3_0_1_linear_2_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.6015625.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_6_8_5_1_2_1_maxpool_3_3_0_1_linear_3_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_6_8_5_1_2_1_maxpool_3_3_0_1_linear_4_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_6_8_5_1_2_1_maxpool_5_5_0_1_linear_1_500_500\"...\n",
      "No checkpoint found at cp_7_conv_6_8_5_1_2_1_maxpool_5_5_0_1_linear_1_500_500.pth. Starting from scratch.\n",
      "Invalid form, skipping now...\n",
      "NaN detected in cnn outputs.\n",
      "\n",
      "Starting training for \"cnn_7, conv_6_8_5_1_2_1_maxpool_5_5_0_1_linear_2_500_500\"...\n",
      "No checkpoint found at cp_7_conv_6_8_5_1_2_1_maxpool_5_5_0_1_linear_2_500_500.pth. Starting from scratch.\n",
      "Invalid form, skipping now...\n",
      "NaN detected in cnn outputs.\n",
      "\n",
      "Starting training for \"cnn_7, conv_6_8_5_1_2_1_maxpool_5_5_0_1_linear_3_500_500\"...\n",
      "No checkpoint found at cp_7_conv_6_8_5_1_2_1_maxpool_5_5_0_1_linear_3_500_500.pth. Starting from scratch.\n",
      "Invalid form, skipping now...\n",
      "NaN detected in cnn outputs.\n",
      "\n",
      "Starting training for \"cnn_7, conv_6_8_5_1_2_1_maxpool_5_5_0_1_linear_4_500_500\"...\n",
      "No checkpoint found at cp_7_conv_6_8_5_1_2_1_maxpool_5_5_0_1_linear_4_500_500.pth. Starting from scratch.\n",
      "Invalid form, skipping now...\n",
      "NaN detected in cnn outputs.\n",
      "\n",
      "Starting training for \"cnn_7, conv_1_16_5_1_2_1_maxpool_3_3_0_1_linear_1_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_1_16_5_1_2_1_maxpool_3_3_0_1_linear_2_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_1_16_5_1_2_1_maxpool_3_3_0_1_linear_3_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.9609375.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_1_16_5_1_2_1_maxpool_3_3_0_1_linear_4_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.8125.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_1_16_5_1_2_1_maxpool_5_5_0_1_linear_1_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_1_16_5_1_2_1_maxpool_5_5_0_1_linear_2_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.55078125.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_1_16_5_1_2_1_maxpool_5_5_0_1_linear_3_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.578125.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_1_16_5_1_2_1_maxpool_5_5_0_1_linear_4_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_2_16_5_1_2_1_maxpool_3_3_0_1_linear_1_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.9375.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_2_16_5_1_2_1_maxpool_3_3_0_1_linear_2_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_2_16_5_1_2_1_maxpool_3_3_0_1_linear_3_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_2_16_5_1_2_1_maxpool_3_3_0_1_linear_4_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_2_16_5_1_2_1_maxpool_5_5_0_1_linear_1_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.9375.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_2_16_5_1_2_1_maxpool_5_5_0_1_linear_2_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.93359375.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_2_16_5_1_2_1_maxpool_5_5_0_1_linear_3_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_2_16_5_1_2_1_maxpool_5_5_0_1_linear_4_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_3_16_5_1_2_1_maxpool_3_3_0_1_linear_1_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.9375.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_3_16_5_1_2_1_maxpool_3_3_0_1_linear_2_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_3_16_5_1_2_1_maxpool_3_3_0_1_linear_3_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_3_16_5_1_2_1_maxpool_3_3_0_1_linear_4_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_3_16_5_1_2_1_maxpool_5_5_0_1_linear_1_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_3_16_5_1_2_1_maxpool_5_5_0_1_linear_2_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_3_16_5_1_2_1_maxpool_5_5_0_1_linear_3_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_3_16_5_1_2_1_maxpool_5_5_0_1_linear_4_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_4_16_5_1_2_1_maxpool_3_3_0_1_linear_1_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_4_16_5_1_2_1_maxpool_3_3_0_1_linear_2_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_4_16_5_1_2_1_maxpool_3_3_0_1_linear_3_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_4_16_5_1_2_1_maxpool_3_3_0_1_linear_4_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_4_16_5_1_2_1_maxpool_5_5_0_1_linear_1_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_4_16_5_1_2_1_maxpool_5_5_0_1_linear_2_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_4_16_5_1_2_1_maxpool_5_5_0_1_linear_3_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_4_16_5_1_2_1_maxpool_5_5_0_1_linear_4_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_5_16_5_1_2_1_maxpool_3_3_0_1_linear_1_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_5_16_5_1_2_1_maxpool_3_3_0_1_linear_2_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 12.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 12 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_5_16_5_1_2_1_maxpool_3_3_0_1_linear_3_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 10.\n",
      "\tPrevious reached accuracy: 1.0.\n",
      "\tSkipping training: accuracy of 1 already achieved.\n",
      "\n",
      "Starting training for \"cnn_7, conv_5_16_5_1_2_1_maxpool_3_3_0_1_linear_4_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 10.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 10 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_5_16_5_1_2_1_maxpool_5_5_0_1_linear_1_500_500\"...\n",
      "No checkpoint found at cp_7_conv_5_16_5_1_2_1_maxpool_5_5_0_1_linear_1_500_500.pth. Starting from scratch.\n",
      "Invalid form, skipping now...\n",
      "NaN detected in cnn outputs.\n",
      "\n",
      "Starting training for \"cnn_7, conv_5_16_5_1_2_1_maxpool_5_5_0_1_linear_2_500_500\"...\n",
      "No checkpoint found at cp_7_conv_5_16_5_1_2_1_maxpool_5_5_0_1_linear_2_500_500.pth. Starting from scratch.\n",
      "Invalid form, skipping now...\n",
      "NaN detected in cnn outputs.\n",
      "\n",
      "Starting training for \"cnn_7, conv_5_16_5_1_2_1_maxpool_5_5_0_1_linear_3_500_500\"...\n",
      "No checkpoint found at cp_7_conv_5_16_5_1_2_1_maxpool_5_5_0_1_linear_3_500_500.pth. Starting from scratch.\n",
      "Invalid form, skipping now...\n",
      "NaN detected in cnn outputs.\n",
      "\n",
      "Starting training for \"cnn_7, conv_5_16_5_1_2_1_maxpool_5_5_0_1_linear_4_500_500\"...\n",
      "No checkpoint found at cp_7_conv_5_16_5_1_2_1_maxpool_5_5_0_1_linear_4_500_500.pth. Starting from scratch.\n",
      "Invalid form, skipping now...\n",
      "NaN detected in cnn outputs.\n",
      "\n",
      "Starting training for \"cnn_7, conv_6_16_5_1_2_1_maxpool_3_3_0_1_linear_1_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 10.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 10 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_6_16_5_1_2_1_maxpool_3_3_0_1_linear_2_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 10.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 10 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_6_16_5_1_2_1_maxpool_3_3_0_1_linear_3_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 10.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 10 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_6_16_5_1_2_1_maxpool_3_3_0_1_linear_4_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 10.\n",
      "\tPrevious reached accuracy: 0.5.\n",
      "\tSkipping training: maximum allowed epoch of 10 already exhausted.\n",
      "\n",
      "Starting training for \"cnn_7, conv_6_16_5_1_2_1_maxpool_5_5_0_1_linear_1_500_500\"...\n",
      "No checkpoint found at cp_7_conv_6_16_5_1_2_1_maxpool_5_5_0_1_linear_1_500_500.pth. Starting from scratch.\n",
      "Invalid form, skipping now...\n",
      "NaN detected in cnn outputs.\n",
      "\n",
      "Starting training for \"cnn_7, conv_6_16_5_1_2_1_maxpool_5_5_0_1_linear_2_500_500\"...\n",
      "No checkpoint found at cp_7_conv_6_16_5_1_2_1_maxpool_5_5_0_1_linear_2_500_500.pth. Starting from scratch.\n",
      "Invalid form, skipping now...\n",
      "NaN detected in cnn outputs.\n",
      "\n",
      "Starting training for \"cnn_7, conv_6_16_5_1_2_1_maxpool_5_5_0_1_linear_3_500_500\"...\n",
      "No checkpoint found at cp_7_conv_6_16_5_1_2_1_maxpool_5_5_0_1_linear_3_500_500.pth. Starting from scratch.\n",
      "Invalid form, skipping now...\n",
      "NaN detected in cnn outputs.\n",
      "\n",
      "Starting training for \"cnn_7, conv_6_16_5_1_2_1_maxpool_5_5_0_1_linear_4_500_500\"...\n",
      "No checkpoint found at cp_7_conv_6_16_5_1_2_1_maxpool_5_5_0_1_linear_4_500_500.pth. Starting from scratch.\n",
      "Invalid form, skipping now...\n",
      "NaN detected in cnn outputs.\n",
      "\n",
      "Rank 1: conv_5_16_5_1_2_1_maxpool_3_3_0_1_linear_3_500_500, Accuracy: 1.0, Epoch: 10\n",
      "Rank 2: conv_4_8_5_1_2_1_maxpool_5_5_0_1_linear_2_500_500, Accuracy: 1.0, Epoch: 12\n",
      "Rank 3: conv_1_8_5_1_2_1_maxpool_3_3_0_1_linear_2_500_500, Accuracy: 1.0, Epoch: 29\n",
      "Rank 4: conv_1_8_5_1_2_1_maxpool_5_5_0_1_linear_4_500_500, Accuracy: 0.98828125, Epoch: 29\n",
      "Rank 5: conv_1_8_5_1_2_1_maxpool_5_5_0_1_linear_3_500_500, Accuracy: 0.98046875, Epoch: 29\n",
      "Rank 6: conv_1_8_5_1_2_1_maxpool_3_3_0_1_linear_3_500_500, Accuracy: 0.96484375, Epoch: 29\n",
      "Rank 7: conv_1_16_5_1_2_1_maxpool_3_3_0_1_linear_3_500_500, Accuracy: 0.9609375, Epoch: 12\n",
      "Rank 8: conv_2_8_5_1_2_1_maxpool_5_5_0_1_linear_2_500_500, Accuracy: 0.94921875, Epoch: 29\n",
      "Rank 9: conv_2_8_5_1_2_1_maxpool_3_3_0_1_linear_2_500_500, Accuracy: 0.9453125, Epoch: 29\n",
      "Rank 10: conv_2_8_5_1_2_1_maxpool_5_5_0_1_linear_1_500_500, Accuracy: 0.9453125, Epoch: 29\n",
      "Rank 11: conv_2_8_5_1_2_1_maxpool_5_5_0_1_linear_3_500_500, Accuracy: 0.9453125, Epoch: 29\n",
      "Rank 12: conv_3_8_5_1_2_1_maxpool_3_3_0_1_linear_2_500_500, Accuracy: 0.9453125, Epoch: 29\n",
      "Rank 13: conv_2_8_5_1_2_1_maxpool_3_3_0_1_linear_3_500_500, Accuracy: 0.94140625, Epoch: 29\n",
      "Rank 14: conv_2_16_5_1_2_1_maxpool_3_3_0_1_linear_1_500_500, Accuracy: 0.9375, Epoch: 12\n",
      "Rank 15: conv_2_16_5_1_2_1_maxpool_5_5_0_1_linear_1_500_500, Accuracy: 0.9375, Epoch: 12\n",
      "Rank 16: conv_3_16_5_1_2_1_maxpool_3_3_0_1_linear_1_500_500, Accuracy: 0.9375, Epoch: 12\n",
      "Rank 17: conv_2_16_5_1_2_1_maxpool_5_5_0_1_linear_2_500_500, Accuracy: 0.93359375, Epoch: 12\n",
      "Rank 18: conv_3_8_5_1_2_1_maxpool_3_3_0_1_linear_3_500_500, Accuracy: 0.93359375, Epoch: 29\n",
      "Rank 19: conv_2_8_5_1_2_1_maxpool_3_3_0_1_linear_4_500_500, Accuracy: 0.91796875, Epoch: 29\n",
      "Rank 20: conv_2_8_5_1_2_1_maxpool_5_5_0_1_linear_4_500_500, Accuracy: 0.91015625, Epoch: 29\n",
      "Pickle file top_cnn_6 not found. Skipping load...\n",
      "Starting training for \"cnn_6, conv_5_16_5_1_2_1_maxpool_3_3_0_1_linear_3_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 728.\n",
      "\tPrevious reached accuracy: 0.90625.\n",
      "Checkpoint saved for epoch 800, reached accuracy: 0.91015625\n",
      "Checkpoint saved for epoch 900, reached accuracy: 0.9140625\n",
      "Checkpoint saved for epoch 1000, reached accuracy: 0.9140625\n",
      "Checkpoint saved for epoch 1100, reached accuracy: 0.9140625\n",
      "Checkpoint saved for epoch 1200, reached accuracy: 0.9140625\n",
      "Checkpoint saved for epoch 1300, reached accuracy: 0.9140625\n",
      "Checkpoint saved for epoch 1400, reached accuracy: 0.9140625\n",
      "Checkpoint saved for epoch 1500, reached accuracy: 0.9140625\n",
      "Checkpoint saved for epoch 1600, reached accuracy: 0.9140625\n",
      "\tMaximum allowed epoch of 1600 reached, aborting training.\n",
      "\n",
      "Starting training for \"cnn_6, conv_4_8_5_1_2_1_maxpool_5_5_0_1_linear_2_500_500\"...\n",
      "Checkpoint loaded. Resuming from epoch 500.\n",
      "\tPrevious reached accuracy: 0.6640625.\n",
      "Checkpoint saved for epoch 600, reached accuracy: 0.734375\n",
      "Checkpoint saved for epoch 700, reached accuracy: 0.796875\n",
      "Checkpoint saved for epoch 800, reached accuracy: 0.81640625\n",
      "Checkpoint saved for epoch 900, reached accuracy: 0.83984375\n",
      "Checkpoint saved for epoch 1000, reached accuracy: 0.87890625\n",
      "Checkpoint saved for epoch 1100, reached accuracy: 0.8984375\n",
      "Checkpoint saved for epoch 1200, reached accuracy: 0.90234375\n",
      "Checkpoint saved for epoch 1300, reached accuracy: 0.90234375\n",
      "Checkpoint saved for epoch 1400, reached accuracy: 0.90625\n",
      "Checkpoint saved for epoch 1500, reached accuracy: 0.91015625\n",
      "Checkpoint saved for epoch 1600, reached accuracy: 0.91015625\n",
      "\tMaximum allowed epoch of 1600 reached, aborting training.\n",
      "\n",
      "Starting training for \"cnn_6, conv_1_8_5_1_2_1_maxpool_3_3_0_1_linear_2_500_500\"...\n",
      "No checkpoint found at cp_6_conv_1_8_5_1_2_1_maxpool_3_3_0_1_linear_2_500_500.pth. Starting from scratch.\n",
      "Checkpoint saved for epoch 100, reached accuracy: 0.66796875\n",
      "Checkpoint saved for epoch 200, reached accuracy: 0.7421875\n",
      "Checkpoint saved for epoch 300, reached accuracy: 0.74609375\n",
      "Checkpoint saved for epoch 400, reached accuracy: 0.75\n",
      "Checkpoint saved for epoch 500, reached accuracy: 0.75\n",
      "Checkpoint saved for epoch 600, reached accuracy: 0.80859375\n",
      "Checkpoint saved for epoch 700, reached accuracy: 0.80859375\n",
      "Checkpoint saved for epoch 800, reached accuracy: 0.81640625\n",
      "Checkpoint saved for epoch 900, reached accuracy: 0.84375\n",
      "Checkpoint saved for epoch 1000, reached accuracy: 0.8515625\n",
      "Checkpoint saved for epoch 1100, reached accuracy: 0.87109375\n",
      "Checkpoint saved for epoch 1200, reached accuracy: 0.87890625\n",
      "Checkpoint saved for epoch 1300, reached accuracy: 0.89453125\n",
      "Checkpoint saved for epoch 1400, reached accuracy: 0.89453125\n",
      "Checkpoint saved for epoch 1500, reached accuracy: 0.89453125\n",
      "Checkpoint saved for epoch 1600, reached accuracy: 0.9140625\n",
      "\tMaximum allowed epoch of 1600 reached, aborting training.\n",
      "\n",
      "Starting training for \"cnn_6, conv_1_8_5_1_2_1_maxpool_5_5_0_1_linear_4_500_500\"...\n",
      "No checkpoint found at cp_6_conv_1_8_5_1_2_1_maxpool_5_5_0_1_linear_4_500_500.pth. Starting from scratch.\n",
      "Checkpoint saved for epoch 100, reached accuracy: 0.58203125\n",
      "Checkpoint saved for epoch 200, reached accuracy: 0.64453125\n",
      "Checkpoint saved for epoch 300, reached accuracy: 0.703125\n",
      "Checkpoint saved for epoch 400, reached accuracy: 0.7265625\n",
      "Checkpoint saved for epoch 500, reached accuracy: 0.7421875\n",
      "Checkpoint saved for epoch 600, reached accuracy: 0.7421875\n",
      "Checkpoint saved for epoch 700, reached accuracy: 0.8125\n",
      "Checkpoint saved for epoch 800, reached accuracy: 0.85546875\n",
      "Checkpoint saved for epoch 900, reached accuracy: 0.8671875\n",
      "Checkpoint saved for epoch 1000, reached accuracy: 0.8671875\n",
      "Checkpoint saved for epoch 1100, reached accuracy: 0.87890625\n",
      "Checkpoint saved for epoch 1200, reached accuracy: 0.88671875\n",
      "Checkpoint saved for epoch 1300, reached accuracy: 0.88671875\n",
      "Checkpoint saved for epoch 1400, reached accuracy: 0.88671875\n",
      "Checkpoint saved for epoch 1500, reached accuracy: 0.88671875\n",
      "Checkpoint saved for epoch 1600, reached accuracy: 0.88671875\n",
      "\tMaximum allowed epoch of 1600 reached, aborting training.\n",
      "\n",
      "Starting training for \"cnn_6, conv_1_8_5_1_2_1_maxpool_5_5_0_1_linear_3_500_500\"...\n",
      "No checkpoint found at cp_6_conv_1_8_5_1_2_1_maxpool_5_5_0_1_linear_3_500_500.pth. Starting from scratch.\n",
      "Checkpoint saved for epoch 100, reached accuracy: 0.578125\n",
      "Checkpoint saved for epoch 200, reached accuracy: 0.75\n",
      "Checkpoint saved for epoch 300, reached accuracy: 0.75\n",
      "Checkpoint saved for epoch 400, reached accuracy: 0.75390625\n",
      "Checkpoint saved for epoch 500, reached accuracy: 0.8046875\n",
      "Checkpoint saved for epoch 600, reached accuracy: 0.84375\n",
      "Checkpoint saved for epoch 700, reached accuracy: 0.84375\n",
      "Checkpoint saved for epoch 800, reached accuracy: 0.85546875\n",
      "Checkpoint saved for epoch 900, reached accuracy: 0.85546875\n",
      "Checkpoint saved for epoch 1000, reached accuracy: 0.8671875\n",
      "Checkpoint saved for epoch 1100, reached accuracy: 0.8828125\n",
      "Checkpoint saved for epoch 1200, reached accuracy: 0.8828125\n",
      "Checkpoint saved for epoch 1300, reached accuracy: 0.8828125\n",
      "Checkpoint saved for epoch 1400, reached accuracy: 0.8984375\n",
      "Checkpoint saved for epoch 1500, reached accuracy: 0.90234375\n",
      "Checkpoint saved for epoch 1600, reached accuracy: 0.9140625\n",
      "\tMaximum allowed epoch of 1600 reached, aborting training.\n",
      "\n",
      "Starting training for \"cnn_6, conv_1_8_5_1_2_1_maxpool_3_3_0_1_linear_3_500_500\"...\n",
      "No checkpoint found at cp_6_conv_1_8_5_1_2_1_maxpool_3_3_0_1_linear_3_500_500.pth. Starting from scratch.\n",
      "Checkpoint saved for epoch 100, reached accuracy: 0.578125\n",
      "Checkpoint saved for epoch 200, reached accuracy: 0.74609375\n",
      "Checkpoint saved for epoch 300, reached accuracy: 0.74609375\n",
      "Checkpoint saved for epoch 400, reached accuracy: 0.80859375\n",
      "Checkpoint saved for epoch 500, reached accuracy: 0.83203125\n",
      "Checkpoint saved for epoch 600, reached accuracy: 0.84765625\n",
      "Checkpoint saved for epoch 700, reached accuracy: 0.91015625\n",
      "Checkpoint saved for epoch 800, reached accuracy: 0.9140625\n",
      "Checkpoint saved for epoch 900, reached accuracy: 0.9140625\n",
      "Checkpoint saved for epoch 1000, reached accuracy: 0.9140625\n",
      "Checkpoint saved for epoch 1100, reached accuracy: 0.9140625\n",
      "Checkpoint saved for epoch 1200, reached accuracy: 0.9140625\n",
      "Checkpoint saved for epoch 1300, reached accuracy: 0.9140625\n",
      "Checkpoint saved for epoch 1400, reached accuracy: 0.9140625\n",
      "Checkpoint saved for epoch 1500, reached accuracy: 0.9140625\n",
      "Checkpoint saved for epoch 1600, reached accuracy: 0.9140625\n",
      "\tMaximum allowed epoch of 1600 reached, aborting training.\n",
      "\n",
      "Starting training for \"cnn_6, conv_1_16_5_1_2_1_maxpool_3_3_0_1_linear_3_500_500\"...\n",
      "No checkpoint found at cp_6_conv_1_16_5_1_2_1_maxpool_3_3_0_1_linear_3_500_500.pth. Starting from scratch.\n",
      "Checkpoint saved for epoch 100, reached accuracy: 0.65234375\n",
      "Checkpoint saved for epoch 200, reached accuracy: 0.7421875\n",
      "Checkpoint saved for epoch 300, reached accuracy: 0.75\n",
      "Checkpoint saved for epoch 400, reached accuracy: 0.75\n",
      "Checkpoint saved for epoch 500, reached accuracy: 0.75\n",
      "Checkpoint saved for epoch 600, reached accuracy: 0.83203125\n",
      "Checkpoint saved for epoch 700, reached accuracy: 0.88671875\n",
      "Checkpoint saved for epoch 800, reached accuracy: 0.890625\n",
      "Checkpoint saved for epoch 900, reached accuracy: 0.890625\n",
      "Checkpoint saved for epoch 1000, reached accuracy: 0.90234375\n",
      "Checkpoint saved for epoch 1100, reached accuracy: 0.90234375\n",
      "Checkpoint saved for epoch 1200, reached accuracy: 0.90234375\n",
      "Checkpoint saved for epoch 1300, reached accuracy: 0.90234375\n",
      "Checkpoint saved for epoch 1400, reached accuracy: 0.90234375\n",
      "Checkpoint saved for epoch 1500, reached accuracy: 0.90234375\n",
      "Checkpoint saved for epoch 1600, reached accuracy: 0.90234375\n",
      "\tMaximum allowed epoch of 1600 reached, aborting training.\n",
      "\n",
      "Starting training for \"cnn_6, conv_2_8_5_1_2_1_maxpool_5_5_0_1_linear_2_500_500\"...\n",
      "No checkpoint found at cp_6_conv_2_8_5_1_2_1_maxpool_5_5_0_1_linear_2_500_500.pth. Starting from scratch.\n",
      "Checkpoint saved for epoch 100, reached accuracy: 0.68359375\n",
      "Checkpoint saved for epoch 200, reached accuracy: 0.87890625\n",
      "Checkpoint saved for epoch 300, reached accuracy: 0.890625\n",
      "Checkpoint saved for epoch 400, reached accuracy: 0.8984375\n",
      "Checkpoint saved for epoch 500, reached accuracy: 0.90234375\n",
      "Checkpoint saved for epoch 600, reached accuracy: 0.90625\n",
      "Checkpoint saved for epoch 700, reached accuracy: 0.90625\n",
      "Checkpoint saved for epoch 800, reached accuracy: 0.90625\n",
      "Checkpoint saved for epoch 900, reached accuracy: 0.90625\n",
      "Checkpoint saved for epoch 1000, reached accuracy: 0.9140625\n",
      "Checkpoint saved for epoch 1100, reached accuracy: 0.9140625\n",
      "Checkpoint saved for epoch 1200, reached accuracy: 0.9140625\n",
      "Checkpoint saved for epoch 1300, reached accuracy: 0.91796875\n",
      "Checkpoint saved for epoch 1400, reached accuracy: 0.91796875\n",
      "Checkpoint saved for epoch 1500, reached accuracy: 0.91796875\n",
      "Checkpoint saved for epoch 1600, reached accuracy: 0.91796875\n",
      "\tMaximum allowed epoch of 1600 reached, aborting training.\n",
      "\n",
      "Starting training for \"cnn_6, conv_2_8_5_1_2_1_maxpool_3_3_0_1_linear_2_500_500\"...\n",
      "No checkpoint found at cp_6_conv_2_8_5_1_2_1_maxpool_3_3_0_1_linear_2_500_500.pth. Starting from scratch.\n",
      "Checkpoint saved for epoch 100, reached accuracy: 0.55078125\n",
      "Checkpoint saved for epoch 200, reached accuracy: 0.55078125\n",
      "Checkpoint saved for epoch 300, reached accuracy: 0.5625\n",
      "Checkpoint saved for epoch 400, reached accuracy: 0.671875\n",
      "Checkpoint saved for epoch 500, reached accuracy: 0.74609375\n",
      "Checkpoint saved for epoch 600, reached accuracy: 0.765625\n",
      "Checkpoint saved for epoch 700, reached accuracy: 0.84765625\n",
      "Training interrupted. Pausing training...\n",
      "Checkpoint saved for last epoch 754\n",
      "\tFinal accuracy: 0.765625\n",
      "Starting training for \"cnn_6, conv_2_8_5_1_2_1_maxpool_5_5_0_1_linear_1_500_500\"...\n",
      "No checkpoint found at cp_6_conv_2_8_5_1_2_1_maxpool_5_5_0_1_linear_1_500_500.pth. Starting from scratch.\n",
      "Checkpoint saved for epoch 100, reached accuracy: 0.5625\n",
      "Checkpoint saved for epoch 200, reached accuracy: 0.5625\n",
      "Checkpoint saved for epoch 300, reached accuracy: 0.5625\n",
      "Checkpoint saved for epoch 400, reached accuracy: 0.5625\n",
      "Checkpoint saved for epoch 500, reached accuracy: 0.70703125\n"
     ]
    }
   ],
   "source": [
    "root_directory = os.getcwd()\n",
    "# Create folder to store ALL files\n",
    "dynamic_cnn_directory = os.path.join(root_directory, f'Dynamic_CNN_Attack_Files')\n",
    "os.makedirs(dynamic_cnn_directory, exist_ok=True)\n",
    "os.chdir(dynamic_cnn_directory)\n",
    "\n",
    "test_cnns, test_optims, criterion, learning_rate, tot_cnns, cnn_strings = dynamic_cnn_init(cnn_format, def_lr)\n",
    "\n",
    "combined_result_filename = \"combined_result\"\n",
    "\n",
    "combined_max_accuracy= []\n",
    "combined_min_epochs = []\n",
    "combined_best_settings = []\n",
    "\n",
    "try:\n",
    "    with open(combined_result_filename, 'rb') as f:\n",
    "        loaded_data = pickle.load(f)\n",
    "        combined_max_accuracy = loaded_data[\"combined_max_accuracy\"]\n",
    "        combined_min_epochs = loaded_data[\"combined_min_epochs\"]\n",
    "        combined_best_settings = loaded_data[\"combined_best_settings\"]\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\tPickle file {combined_result_filename} not found. combined_result not loaded.\")\n",
    "except EOFError:\n",
    "    print(f\"\\tPickle file {combined_result_filename} not found. combined_result not loaded.\")\n",
    "\n",
    "\n",
    "for cnn_num in range(7, -1, -1):\n",
    "    # Create directory to save pth files\n",
    "    cnn_pt_directory = os.path.join(dynamic_cnn_directory, f'cnn{cnn_num}')\n",
    "    os.makedirs(cnn_pt_directory, exist_ok=True)\n",
    "    os.chdir(cnn_pt_directory)\n",
    "\n",
    "    bit_cnn_result = {}\n",
    "    max_acc = 0\n",
    "    best_strings = []\n",
    "    top_cnns_filename = f\"top_cnn_{cnn_num}\"\n",
    "    check_ranking = 20\n",
    "\n",
    "    try:\n",
    "        with open(top_cnns_filename, 'rb') as f:\n",
    "            loaded_data = pickle.load(f)\n",
    "            top_cnns = loaded_data[\"top_cnns\"]\n",
    "            base_min_epoch = loaded_data[\"base_min_epoch\"]\n",
    "            check_ranking = loaded_data[\"check_ranking\"]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Pickle file {top_cnns_filename} not found. Skipping load...\")\n",
    "        base_min_epoch = max_allowed_epoch[str(cnn_num)]\n",
    "    except EOFError:\n",
    "        top_cnns = {}\n",
    "        base_min_epoch = max_allowed_epoch[str(cnn_num)]\n",
    "    \n",
    "    min_epoch = base_min_epoch\n",
    "\n",
    "    while max_acc != 1:\n",
    "        for i in range(tot_cnns):\n",
    "            if min_epoch < base_min_epoch:\n",
    "                epoch, accuracy = train_cnns(cnn_num, i, test_cnns[i], test_optims[i], cnn_strings[i], criterion, min_epoch)\n",
    "            else:\n",
    "                epoch, accuracy = train_cnns(cnn_num, i, test_cnns[i], test_optims[i], cnn_strings[i], criterion, base_min_epoch)\n",
    "            bit_cnn_result[cnn_strings[i]] = {\n",
    "                \"epoch\": epoch,\n",
    "                \"accuracy\": accuracy,\n",
    "                \"index\": i \n",
    "            }\n",
    "\n",
    "            if epoch > min_epoch:\n",
    "                base_min_epoch = epoch\n",
    "                min_epoch = epoch\n",
    "\n",
    "            if accuracy > max_acc or (accuracy == max_acc and epoch <= min_epoch):\n",
    "                max_acc = accuracy\n",
    "                min_epoch = epoch\n",
    "        \n",
    "        top_cnns = sorted(bit_cnn_result.items(), key=lambda x: (-x[1][\"accuracy\"], x[1][\"epoch\"]))[:check_ranking]\n",
    "        ranking_dict = {rank + 1: item[0] for rank, item in enumerate(top_cnns)}\n",
    "        \n",
    "        new_test_cnns = []\n",
    "        new_optims = []\n",
    "        new_cnn_strings = []\n",
    "\n",
    "        for rank, (cnn_string, data) in enumerate(top_cnns, start=1):\n",
    "            print(f\"Rank {rank}: {cnn_string}, Accuracy: {data['accuracy']}, Epoch: {data['epoch']}\")\n",
    "            new_test_cnns.append(test_cnns[data['index']])\n",
    "            new_optims.append(test_optims[data['index']])\n",
    "            new_cnn_strings.append(cnn_strings[data['index']])\n",
    "\n",
    "        test_cnns = new_test_cnns\n",
    "        test_optims = new_optims\n",
    "        cnn_strings = new_cnn_strings\n",
    "        tot_cnns = check_ranking\n",
    "\n",
    "        if min_epoch == base_min_epoch:\n",
    "            print(f\"\\tCurrent max epoch: {base_min_epoch} insufficient. Increasing epochs...\")\n",
    "            base_min_epoch *= 2\n",
    "            min_epoch *= 2\n",
    "            print(f\"\\tIncreased max epochs to: {base_min_epoch}...\")\n",
    "            if check_ranking > 5:\n",
    "                check_ranking // 2\n",
    "        \n",
    "        with open(top_cnns_filename, 'wb') as f:\n",
    "            pickle.dump({\"top_cnns\": top_cnns, \"base_min_epoch\": base_min_epoch, \"check_ranking\": check_ranking}, f)\n",
    "\n",
    "    for rank, (cnn_string, data) in enumerate(top_cnns, start=1):\n",
    "        if data['accuracy'] == 1:\n",
    "            best_strings.append(cnn_string)\n",
    "\n",
    "    combined_max_accuracy.append(max_acc)\n",
    "    combined_min_epochs.append(min_epoch)\n",
    "    combined_best_settings.append(best_strings)\n",
    "    \n",
    "    os.chdir(dynamic_cnn_directory)\n",
    "    with open(combined_result_filename, 'wb') as f:\n",
    "        pickle.dump({\"combined_max_accuracy\": combined_max_accuracy,\n",
    "                     \"combined_min_epochs\": combined_min_epochs,\n",
    "                     \"combined_best_settings\": combined_best_settings}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the CNNs\n",
    "\n",
    "Using the resulting CNNs saved in the .pth files, we test each CNNs using the provided power traces.\n",
    "\n",
    "Currently the testing data is a subset of the training data. In the future, if we can generate more data, we will be able to use separate datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Run evaluation using testing data\n",
    "# CURRENTLY USING TRAINING DATA TO TEST DATA: use separate dataset in the future\n",
    "for i in range(7,-1,-1):\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)\n",
    "        cnn.load_state_dict(checkpoint['cnn_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        print(f\"Checkpoint loaded. Resuming from epoch {start_epoch}\")\n",
    "        reached_target_acc = checkpoint['reached_acc']\n",
    "        print(f\"Previous reached accuracy: {reached_target_acc}\")\n",
    "        if reached_target_acc != 1:\n",
    "            print(f\"cnn_{i}\\\" did not reach accuracy of 1, skipping evaluation.\\n\\n\")\n",
    "            continue\n",
    "    except FileNotFoundError:\n",
    "        print(f\"No checkpoint found for \\\"cnn_{i}\\\". Starting from scratch.\")\n",
    "\n",
    "    cnns[i].eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloaders[i]:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # Add dimensions for channels and width\n",
    "            inputs = inputs.unsqueeze(1).unsqueeze(-1)\n",
    "            outputs = cnns[i](inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Accuracy: {100 * correct / total:.2f}%\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
