{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN attack via ResNet using Skywater non-linearized data\n",
    "\n",
    "The goal of this notebook is to correctly preprocess the given data as tensors that can be used to train ResNet101."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader.py\n",
    "# Necessary imports\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataloaders\n",
    "Used files from Kareem's GitHub repo. I may have made mistakes in sampling the data, so feel free to change anything that has been configured incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader.py\n",
    "# Returns list of files with given format \n",
    "def get_files(directory, format, digital_index=0):\n",
    "\n",
    "    format = re.compile(format)\n",
    "    files = os.listdir(directory)\n",
    "\n",
    "    #file_dict = {}\n",
    "    file_list = [] # fname, fpath, label\n",
    "\n",
    "    for fname in files:\n",
    "        if match := format.match(fname):\n",
    "            fpath = os.path.join(directory, fname)\n",
    "\n",
    "            dvalue = int(match.groups()[digital_index])\n",
    "            \n",
    "            file_list.append((fname, fpath, dvalue))\n",
    "\n",
    "            #if dvalue in file_dict:\n",
    "            #    file_dict[dvalue].append(fpath)\n",
    "            #else:\n",
    "            #    file_dict[dvalue] = [fpath]\n",
    "\n",
    "    return file_list #file_dict, file_path\n",
    "\n",
    "# Creates dataset with given traces\n",
    "class TraceDataset(Dataset):\n",
    "    cached_traces = {}\n",
    "    trace_list    = []\n",
    "\n",
    "    def __init__(self, file_list, cache=True):\n",
    "        self.file_list = file_list\n",
    "        self.cache     = cache\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        fname, fpath, label = self.file_list[index]\n",
    "        label = self.process_label(label)\n",
    "\n",
    "        if self.cache and fname in self.cached_traces:\n",
    "            return self.cached_traces[fname], label\n",
    "        else:\n",
    "            return self.load_trace(fname, fpath), label\n",
    "\n",
    "    def get_info(self, index):\n",
    "        return self.file_list[index]\n",
    "\n",
    "    def load_trace(self, fname, fpath):\n",
    "        with open(fpath, 'r') as file:\n",
    "            header = file.readline()\n",
    "            #time_arr = []\n",
    "            valu_arr = []\n",
    "\n",
    "            for line in file.readlines():\n",
    "                time, value = line.strip().split()\n",
    "                #time_arr.append(np.float32(time))\n",
    "                valu_arr.append(np.float32(value))\n",
    "\n",
    "        trace = np.array(valu_arr, dtype=np.float32)\n",
    "\n",
    "        if self.cache: \n",
    "            self.cached_traces[fname] = trace\n",
    "            self.trace_list.append(trace)\n",
    "\n",
    "        return trace\n",
    "    \n",
    "    def process_label(self, label): return label\n",
    "\n",
    "    def cache_all(self):\n",
    "        assert self.cache == True\n",
    "\n",
    "        print(\"Caching all traces\")\n",
    "        for fname, fpath, label in self.file_list:\n",
    "            self.load_trace(fname, fpath)\n",
    "        print(\"DONE Caching all traces\")\n",
    "\n",
    "class TraceDatasetBW(TraceDataset):\n",
    "    def __init__(self, file_list, bit_select, cache=True):\n",
    "        self.bit_mask = 1 << bit_select\n",
    "        super().__init__(file_list, cache=cache)\n",
    "\n",
    "    def process_label(self, label):\n",
    "        return 1 if label & self.bit_mask else 0\n",
    "\n",
    "class TraceDatasetBuilder:\n",
    "    def __init__(self, adc_bitwidth=8, cache=True):\n",
    "        self.file_list        = []\n",
    "        self.cache = cache\n",
    "        self.adc_bits = adc_bitwidth\n",
    "\n",
    "        self.dataset = None\n",
    "        self.dataloader = None\n",
    "        self.datasets = []\n",
    "        self.dataloaders = []\n",
    "\n",
    "    def add_files(self, directory, format, label_group):\n",
    "        ''' Builds list of powertrace files\n",
    "        Inputs:\n",
    "            directory   : folder to search for files\n",
    "            format      : regular expression to match filenames\n",
    "            label_index : group index for digital output label corresponding to trace\n",
    "        Outputs:\n",
    "            list        : [(file_name, file_path, label) ... ]\n",
    "        '''\n",
    "        format = re.compile(format)\n",
    "        fnames = os.listdir(directory)\n",
    "\n",
    "        for fname in fnames:\n",
    "            if match := format.match(fname):\n",
    "                fpath = os.path.join(directory, fname)\n",
    "                dvalue = int(match.groups()[label_group])\n",
    "\n",
    "                self.file_list.append((fname, fpath, dvalue))\n",
    "\n",
    "    def build(self):\n",
    "        self.dataset = TraceDataset(self.file_list, cache=self.cache)\n",
    "        for b in range(self.adc_bits):\n",
    "            self.datasets.append(TraceDatasetBW(self.file_list, b, cache=self.cache))\n",
    "\n",
    "        if self.cache:\n",
    "            self.dataset.cache_all()\n",
    "\n",
    "    def build_dataloaders(self, **kwargs): # batch_size=256, shuffle=True\n",
    "        self.dataloader = DataLoader(self.dataset, **kwargs)\n",
    "        self.dataloaders = [DataLoader(dataset, **kwargs) for dataset in self.datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching all traces\n",
      "DONE Caching all traces\n"
     ]
    }
   ],
   "source": [
    "# Create dataloaders\n",
    "pwd = os.getcwd()\n",
    "# print(pwd)\n",
    "# proj_dir = os.path.dirname(os.path.dirname(pwd))\n",
    "# print(proj_dir)\n",
    "# data_dir = os.path.join(pwd, 'analog', 'outfiles')\n",
    "\n",
    "builder  = TraceDatasetBuilder(adc_bitwidth=8, cache=True)\n",
    "builder.add_files(os.path.join(pwd, 'sky_Dec_18_2151'), \"sky_d(\\\\d+)_.*\\\\.txt\", 0)\n",
    "builder.build()\n",
    "builder.build_dataloaders(batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for ResNet\n",
    "Section where the initial imports and variables for ResNet is set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet101 code\n",
    "# Necessary imports\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "from torchvision.models import resnet101, ResNet101_Weights\n",
    "import numpy as np\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training ResNet101\n",
    "Using pretrained ResNet101, we train each CNN until all of them reaches an accuracy of 1.\n",
    "\n",
    "I'm not sure if aiming for an accuracy of 1 is beneficial, as it is just overfitting the model to the training data. A more realistic value may be 0.99, but will set it to 1 for the current simulated environment.\n",
    "\n",
    "The training function automatically reduces the learning rate used in Adam based on target accuracy. Currently testing different values and decrease rates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for \"cnn_7\"...\n",
      "Checkpoint loaded. Resuming from epoch 9\n",
      "Previous reached accuracy: 1.0\n",
      "Skipping training for \"cnn_7\", already reached accuracy of 1....\n",
      "\n",
      "\n",
      "Starting training for \"cnn_6\"...\n",
      "Checkpoint loaded. Resuming from epoch 6\n",
      "Previous reached accuracy: 0.953125\n",
      "Updated target accuracy: 0.99\n",
      "Updated learning rate: 2.5e-05\n",
      "TRAINING: cnn[6], Epoch 7, Loss: 0.5197134613990784\n",
      "TRAINING: cnn[6], Epoch 7, Accuracy: 0.97265625\n",
      "Checkpoint saved for epoch 7\n",
      "TRAINING: cnn[6], Epoch 8, Loss: 0.4630599915981293\n",
      "TRAINING: cnn[6], Epoch 8, Accuracy: 0.98046875\n",
      "Checkpoint saved for epoch 8\n",
      "TRAINING: cnn[6], Epoch 9, Loss: 0.408481240272522\n",
      "TRAINING: cnn[6], Epoch 9, Accuracy: 0.98828125\n",
      "Checkpoint saved for epoch 9\n"
     ]
    }
   ],
   "source": [
    "cnns = []\n",
    "dataloaders = builder.dataloaders \n",
    "timestamp   = datetime.datetime.now().strftime('%Y%m%d_%H%M')\n",
    "\n",
    "'''\n",
    "plt.ion()\n",
    "figs, axs = plt.subplots(2)\n",
    "axs[0].set_title(\"Loss\")\n",
    "axs[1].set_title(\"Accuracy\")\n",
    "'''\n",
    "\n",
    "# Create CNN per dataset\n",
    "for i in range(7,-1,-1):\n",
    "    print(f\"Starting training for \\\"cnn_{i}\\\"...\")\n",
    "    # Model: ResNet101, pretrained=true, using ResNet101_Weights.DEFAULT for up-to-date values\n",
    "    cnn = resnet101(weights=ResNet101_Weights.DEFAULT)\n",
    "    cnn.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    cnn.fc = nn.Linear(cnn.fc.in_features, 2)\n",
    "    # Loss function: not specified in paper, used Cross Entropy Loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Optimizer: not specified in paper, used Adam\n",
    "    # Target accuracies to update the learning rate\n",
    "    # Use different values if needed\n",
    "    target_acc = [0.90, 0.95, 0.99, 0.995]\n",
    "    target_acc_index = 0\n",
    "    learning_rate = 1e-4\n",
    "    optimizer = optim.Adam(cnn.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Parameters for training CNN\n",
    "    num_epochs = 150\n",
    "    max_grad_norm = 1.0  # Gradient clipping threshold\n",
    "    learning_rate = 1e-4  # Reduced learning rate for stability, inital value = 1e-4\n",
    "    '''\n",
    "    # Paramters for plot, may erase if not used\n",
    "    loss_arr = []  # Array used to store loss values over epoches\n",
    "    acc_arr  = []  # Array used to store accuracy values over epoches\n",
    "    loss_g = None\n",
    "    acc_g  = None\n",
    "    '''\n",
    "\n",
    "    # Append CNN to cnns array\n",
    "    cnns.append(cnn)\n",
    "\n",
    "    # Create checkpoint to save progress\n",
    "    checkpoint_path = f\"resnet101_checkpoint_{i}.pth\"\n",
    "    start_epoch = 0\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        cnn = cnn.to(device)\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    # Try to load .pth file\n",
    "    # NEED TO ADD FUNCTIONALITY TO CHECK INTEGRITY OF .pth FILE\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)\n",
    "        cnn.load_state_dict(checkpoint['cnn_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        print(f\"Checkpoint loaded. Resuming from epoch {start_epoch}\")\n",
    "        reached_target_acc = checkpoint['reached_acc']\n",
    "        print(f\"Previous reached accuracy: {reached_target_acc}\")\n",
    "        if reached_target_acc == 1:\n",
    "            print(f\"Skipping training for \\\"cnn_{i}\\\", already reached accuracy of 1.\\n\\n\")\n",
    "            continue\n",
    "        while target_acc_index < len(target_acc) - 1 and target_acc[target_acc_index] < reached_target_acc:\n",
    "            target_acc_index += 1\n",
    "            learning_rate /= 2\n",
    "        print(f\"Updated target accuracy: {target_acc[target_acc_index]}\")\n",
    "        print(f\"Updated learning rate: {learning_rate}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"No checkpoint found for \\\"cnn_{i}\\\". Starting from scratch.\")\n",
    "\n",
    "    # Start training\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        correct = 0\n",
    "        cnn.train()\n",
    "\n",
    "        for inputs, labels in dataloaders[i]:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # Add dimensions for channels and width\n",
    "            inputs = inputs.unsqueeze(1).unsqueeze(-1)\n",
    "            optimizer.zero_grad()\n",
    "            output = cnn(inputs)\n",
    "            # Check for NaN in outputs\n",
    "            if torch.isnan(output).any():\n",
    "                print(\"NaN detected in cnn outputs.\")\n",
    "                break\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "            # Check for NaN in loss\n",
    "            if torch.isnan(loss):\n",
    "                print(\"NaN detected in loss. Stopping training.\")\n",
    "                break\n",
    "            # print(f\"Loss: {loss.item()}\")\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(cnn.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            correct += (predicted == labels).sum()\n",
    "        \n",
    "        accuracy = correct / 256\n",
    "        '''\n",
    "        acc_arr.append(accuracy)\n",
    "        loss_arr.append(loss.item())\n",
    "        '''\n",
    "        # Fixed showing accuracy to all epoches cause macbook is slow af\n",
    "        # Change if u are flexing with your computing resources\n",
    "        if (epoch + 1) % 1 == 0:\n",
    "            print(f'TRAINING: cnn[{i}], Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "            print(f'TRAINING: cnn[{i}], Epoch {epoch+1}, Accuracy: {accuracy}')\n",
    "        '''\n",
    "        if epoch % 50 == 0: \n",
    "            if loss_g: loss_g.remove()\n",
    "            if acc_g:  acc_g.remove()\n",
    "            loss_g = axs[0].plot(loss_arr, color='lightgray', linestyle='dotted')[0]\n",
    "            loss_a = axs[1].plot(acc_arr,  color='lightgray', linestyle='dotted')[0]\n",
    "            plt.pause(0.01)\n",
    "        '''\n",
    "\n",
    "        # Save checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'cnn_state_dict': cnn.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'reached_acc': accuracy\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved for epoch {epoch + 1}\")\n",
    "\n",
    "        if accuracy == 1:\n",
    "            print(f\"Reached accuracy of 1. Stopping training for \\\"cnn_{i}\\\".\\n\")\n",
    "            break\n",
    "        # Update learning rate if accuracy reaches target value\n",
    "        # Reducing stepsize accordingly so the optimizer does not overshoot\n",
    "        elif target_acc[target_acc_index] < accuracy and target_acc_index != len(target_acc) - 1:\n",
    "            temp = learning_rate\n",
    "            while target_acc_index <= len(target_acc) - 1 and target_acc[target_acc_index] < accuracy:\n",
    "                target_acc_index += 1\n",
    "                learning_rate = learning_rate / 2\n",
    "            print(f\"Reached target accuracy {target_acc[target_acc_index]}\")\n",
    "            print(f\"Updating learning rate FROM: {temp}, TO: {learning_rate}\")\n",
    "\n",
    "'''\n",
    "    label = f'cnn[{i}]'\n",
    "    axs[0].plot(loss_arr, label=label)\n",
    "    axs[1].plot(acc_arr,  label=label)\n",
    "    axs[0].legend()\n",
    "    axs[1].legend()\n",
    "    plt.pause(0.01)\n",
    "\n",
    "plt.pause(60*10)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
