{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN attack on 5px Analog Linearalized Data using Single CNN\n",
    "\n",
    "The goal of this notebook is to correctly preprocess the given data as tensors that can be used to train a single simple CNN and evaluate its results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader.py\n",
    "# Necessary imports\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common rules\n",
    "1) The BIT numbers are arranged in descending order\n",
    "    1) Thus, in a 8-bit ADC, bit 7 is the MSB and bit 0 is the LSB.\n",
    "2) The ADC numbers are arranged in descending order.\n",
    "    1) Thus, in a 5-pixel case, the ADC with adc_num 4 stores bits [39-32], and ADC with adc_num 0 stores bits [7-0]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter - File Settings\n",
    "Hyperparameters used to define the nature of the power trace files and values used for creating the dataloaders.\n",
    "\n",
    "Double-check if these values match the format of the power trace files; failure to do so may cause unforeseen errors which are extremely difficult to debug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Hyperparameter used to define settings of power trace files.\n",
    "    1) adc_num: number of pixels dataloader we are training/testing is based on. This number of ADCs are created.\n",
    "    2) split_digital: BOOLEAN value. Set to 'True' if the dataloaders need the digital output values of individual ADCs.\n",
    "    Used to save computational power and prevent repetitive operations.\n",
    "    3) normalized_digital: BOOLEAN value. Set to 'True' if the file names provide normalized digital values.\n",
    "    4) train_batch: training dataloader batch size.\n",
    "    5) e_exp: hyperparameter used in normalizing values.\n",
    "    All values are normalized by pow(10, -1*e_exp) to save computational resources.\n",
    "'''\n",
    "adc_num = 5\n",
    "adc_bitwidth = 8\n",
    "split_digital = True\n",
    "normalized_digital = True\n",
    "train_batch = 256\n",
    "e_exp = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter - Trace File Directories\n",
    "All power trace files are assumed to be saved in a folder called 'trace_files' within the root directory of the notebook.\n",
    "\n",
    "Define names of all folders within 'trace_files' to be added to the dataloaders within this hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Hyperparameter used to desginate trace folders to be loaded.\n",
    "    1) sub_folder: BOOLEAN value that is used when trace folders are stored in a specific\n",
    "    folder within the parent directory.\n",
    "    2) sub_folder_name: name of sub-folder\n",
    "    ex) If trace folders are stored in /trace_folders/analog_5px, set sub_folder to True\n",
    "    and sub_folder_name to 'analog_5px'\n",
    "    3) train_trace_folder_names: Add names of folders used for TRAINING dataset to this array.\n",
    "    4) test_trace_folder_names: Add names of folders used for TESTING dataset to this array.\n",
    "    5) trace_type: string that defines the type of trace file to be used. Default is \"lin\" for linearized.\n",
    "    6) file_pattern: RegEx of file names to be checked. Number of groups MUST match 'adc_num'\n",
    "'''\n",
    "sub_folder = True\n",
    "sub_folder_name = 'analog_5px'\n",
    "train_trace_folder_names = ['analog_5px_tt_px']\n",
    "test_trace_folder_names = ['analog_5px_tt_pm']\n",
    "trace_type = \"lin\"\n",
    "file_pattern = trace_type + \"_s\\d+_([0-9]+\\.[0-9]+)_([0-9]+\\.[0-9]+)_([0-9]+\\.[0-9]+)_([0-9]+\\.[0-9]+)_([0-9]+\\.[0-9]+)\\.txt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataloaders\n",
    "Used files from Kareem's GitHub repo. I may have made mistakes in sampling the data, so feel free to change anything that has been configured incorrectly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions\n",
    "process_string: Function that processes trace string values to correct format float64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Function that processes trace string values to correct format float64.\n",
    "It normalizes the values using 'e_exp' hyperparameter and returns correct float64.\n",
    "Inputs:\n",
    "    1) strip_val: value before 1e\n",
    "    2) e_val: 1e exponent value\n",
    "    ex) if input string is \"-2.17498915e-03\", strip_val = \"-2.17498915\", e_val = \"-03\"\n",
    "    3) e_exp: defined hyperparameter. Mormalizes all values by pow(10, -1*e_exp)\n",
    "Returns:\n",
    "    1) new_number: normalized float64 of string\n",
    "'''\n",
    "def process_string(strip_val, e_val, e_exp=e_exp):\n",
    "    # e_diff: exponent difference from base -8 value\n",
    "    e_diff = int(e_val) + e_exp\n",
    "    is_neg = strip_val.startswith('-')\n",
    "\n",
    "    # Remove negative sign for processing\n",
    "    if is_neg:\n",
    "        strip_val = strip_val[1:]\n",
    "    \n",
    "    int_part, dec_part = strip_val.split('.')\n",
    "    new_number = int_part + dec_part\n",
    "\n",
    "    if e_diff < 0:\n",
    "        new_number = '0' * abs(e_diff - 1) + new_number\n",
    "        new_number = '0.' + new_number\n",
    "    else:\n",
    "        if e_diff + 1 < len(new_number):\n",
    "            new_number = new_number[:e_diff + 1] + '.' + new_number[e_diff + 1:]\n",
    "        else:\n",
    "            new_number = new_number.ljust(e_diff + 1, '0')\n",
    "    new_number = np.float64(new_number)\n",
    "\n",
    "    return new_number * -1 if is_neg else new_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trace Classes\n",
    "1) TraceDataset(Dataset): creates dataset of TRACE files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader.py\n",
    "# Creates dataset with given traces\n",
    "class TraceDataset(Dataset):\n",
    "    # cached_traces, trace_list = used to store already created traces\n",
    "    # reused based on digital value, prevents rep calls on load_traces\n",
    "    cached_traces = {}\n",
    "    trace_list    = []\n",
    "\n",
    "    # file_list: list of FILE NAMES that have been converted\n",
    "    # cache: actual traces saved that can be reused\n",
    "    def __init__(self, file_list, cache=True):\n",
    "        self.file_list = file_list\n",
    "        self.cache     = cache\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    # input: index, used for finding file in file_list\n",
    "    def __getitem__(self, index):\n",
    "        fname, fpath, label = self.file_list[index]\n",
    "        label = self.process_label(label)\n",
    "\n",
    "        if self.cache and fname in self.cached_traces:\n",
    "            return self.cached_traces[fname], label\n",
    "        else:\n",
    "            return self.load_trace(fname, fpath), label\n",
    "\n",
    "    def get_info(self, index):\n",
    "        return self.file_list[index]\n",
    "\n",
    "    # opens single trace file, creates valu_arr, patches as tensor\n",
    "    def load_trace(self, fname, fpath):\n",
    "        with open(fpath, 'r') as file:\n",
    "            header = file.readline()\n",
    "            #time_arr = []\n",
    "            valu_arr = []\n",
    "            # Fixed error of float32 incorrectly translating values\n",
    "            for line in file.readlines():\n",
    "                time, value = line.strip().split()\n",
    "                # Edge case: value is \"-0.00000000e+00\" or \"0.00000000e+00\"\n",
    "                # Add more edge cases if needed\n",
    "                if value in [\"-0.00000000e+00\", \"0.00000000e+00\"]:\n",
    "                    valu_arr.append(np.float64(0))\n",
    "                else:\n",
    "                    try:\n",
    "                        match = re.search(r\"(?<=e-)\\d+\", value)\n",
    "                        if match:\n",
    "                            if value[0] == \"-\":\n",
    "                                strip_val = value[0:11]\n",
    "                                strip_val_e = value[12:15]\n",
    "                            else:\n",
    "                                strip_val = value[0:10]\n",
    "                                strip_val_e = value[11:14]\n",
    "                            '''\n",
    "                            # Debugging scripts; do not erase\n",
    "                            print(f\"\\tstrip_val: {strip_val}\")\n",
    "                            print(f\"\\tstrip_val_e: {strip_val_e}\\n\")\n",
    "                            new_val = process_string(strip_val, strip_val_e)\n",
    "                            print(f\"\\tProcessed: {new_val}\")\n",
    "                            print(f\"\\tFloat32 of processed: {np.float32(new_val)}\\n\")\n",
    "                            '''\n",
    "                            valu_arr.append(process_string(strip_val, strip_val_e))\n",
    "                    except ValueError as e:\n",
    "                        print(f\"Error parsing value '{value}': {e}\")\n",
    "        \n",
    "        trace = np.array(valu_arr, dtype=np.float32)\n",
    "        '''\n",
    "        # Debugging scripts; do not erase\n",
    "        print(f\"\\t{trace}\")\n",
    "        print(f\"\\tfname: {fname}\")\n",
    "        print(f\"\\tfpath: {fpath}\\n\")\n",
    "        '''\n",
    "\n",
    "        if self.cache: \n",
    "            self.cached_traces[fname] = trace\n",
    "            self.trace_list.append(trace)\n",
    "\n",
    "        return trace\n",
    "    \n",
    "    # label = PURE digital value as array\n",
    "    def process_label(self, label):\n",
    "        return label\n",
    "\n",
    "    def cache_all(self):\n",
    "        assert self.cache == True\n",
    "        \n",
    "        print(\"Caching all traces\")\n",
    "        for fname, fpath, label in self.file_list:\n",
    "            self.load_trace(fname, fpath)\n",
    "        print(\"DONE Caching all traces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trace Classes\n",
    "2) TraceDatasetBW(TraceDataset): takes a TraceDataset, labels them by bit\n",
    "3) TraceDatasetBuilder: creates final Dataloader with traces and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TraceDatasetBW(TraceDataset):\n",
    "    # bit_select = 0-7, 0 = LSB, 7 = MSB\n",
    "    # adc_select = 0-4, 0 = ADC storing LSB, 4 = ADC storing MSB\n",
    "    def __init__(self, file_list, bit_select, adc_select, cache=True):\n",
    "        # if split_digital, need to create SEPARATE dataloaders PER ADC\n",
    "        if split_digital:\n",
    "            self.adc_num = adc_select\n",
    "            self.bit_mask =  1 << bit_select\n",
    "        else:\n",
    "            self.adc_num = 0\n",
    "            self.bit_mask =  1 << bit_select + adc_select * 8\n",
    "\n",
    "        super().__init__(file_list, cache=cache) \n",
    "    \n",
    "    # Uses bitwise on COMBINED label\n",
    "    def process_label(self, label):\n",
    "        # if split_digital, label = array\n",
    "        if split_digital:\n",
    "            try:\n",
    "                label_num = label[self.adc_num]\n",
    "            except IndexError:\n",
    "                print(f\"\\tInvalid index; this may be caused due to bad hyperparameters.\")\n",
    "                print(f\"\\tadc_num: {adc_num}\")\n",
    "                print(f\"\\tsplit_digital: {split_digital}\")\n",
    "                print(f\"\\tnormalized_digital: {normalized_digital}\")\n",
    "        else:\n",
    "            label_num = label\n",
    "        return 1 if label_num & self.bit_mask else 0\n",
    "\n",
    "class TraceDatasetBuilder:\n",
    "    def __init__(self, adc_bitwidth=8, cache=True):\n",
    "        self.file_list = []\n",
    "        self.cache = cache\n",
    "        self.adc_bits = adc_bitwidth\n",
    "\n",
    "        self.dataset = None\n",
    "        self.dataloader = None\n",
    "        self.datasets = []\n",
    "        self.dataloaders = []\n",
    "\n",
    "    def add_files(self, directory, format, label_group):\n",
    "        ''' Builds list of powertrace files\n",
    "        Inputs:\n",
    "            directory   : folder to search for files\n",
    "            format      : regular expression to match filenames\n",
    "            label_group : group index for digital output label corresponding to trace\n",
    "        Outputs:\n",
    "            list        : [(file_name, file_path, label) ... ]\n",
    "        '''\n",
    "        format = re.compile(format)\n",
    "        fnames = os.listdir(directory)\n",
    "\n",
    "        for fname in fnames:\n",
    "            if match := format.match(fname):\n",
    "                fpath = os.path.join(directory, fname)\n",
    "                # IF split_digital, return ARRAY of digital values\n",
    "                # returns: [[adc_num digital values], ...] (2D array)\n",
    "                # ORDER: MSB values FIRST\n",
    "                if split_digital:\n",
    "                    # dvalue: ordered by FILE NAMING order\n",
    "                    # if normalized, multiply 256 to get original value\n",
    "                    if normalized_digital:\n",
    "                        dvalue = [int(np.float64(i) * 256) for i in match.groups()]\n",
    "                    # else, append as int\n",
    "                    else:\n",
    "                        dvalue = [int(i) for i in match.groups()]\n",
    "                # ELSE, return ARRAY of SINGLE digital values\n",
    "                # returns: [combined digital value, ...] (1D array)\n",
    "                else:\n",
    "                    dvalue = [0]\n",
    "                    # if normalized, multiply 256 to get original value\n",
    "                    if normalized_digital:\n",
    "                        for i in match.groups():\n",
    "                            dvalue[0] = dvalue[0] * 256 + int(np.float64(i) * 256)\n",
    "                    # else, append as int \n",
    "                    else:\n",
    "                        for i in match.groups():\n",
    "                            dvalue[0]  = dvalue[0] * 256 + int(i)\n",
    "                \n",
    "                self.file_list.append((fname, fpath, dvalue))\n",
    "\n",
    "    def build(self):\n",
    "        # dataset = TraceDataset, trace - digital value label ONLY\n",
    "        self.dataset = TraceDataset(self.file_list, cache=self.cache)\n",
    "        # Append dataloaders IN LSB ORDER; dataloader[0] = adc[0], bit[0]\n",
    "        # dataloader[39] = adc[4], bit[7]\n",
    "        # adc_dataloader[adc_num] = [dataloader[adc_num*8+0], dataloader[adc_num*8+1], ..., dataloader[adc_num*8+7]]\n",
    "        for adc in range(adc_num):\n",
    "            for bit in range(self.adc_bits):\n",
    "                self.datasets.append(TraceDatasetBW(self.file_list, bit, adc, cache=self.cache))\n",
    "                \n",
    "        if self.cache:\n",
    "            self.dataset.cache_all()\n",
    "\n",
    "    def build_dataloaders(self, **kwargs): # batch_size=256, shuffle=True\n",
    "        self.dataloader = DataLoader(self.dataset, **kwargs)\n",
    "        self.dataloaders = [DataLoader(dataset, **kwargs) for dataset in self.datasets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create separate dataloaders - Training and Testing\n",
    "Create two separate dataloaders from the provided files.\n",
    "\n",
    "Need some clarification on the specific logic for choosing files(256 per training/testing, mixing both, etc.)\n",
    "\n",
    "There are also some prerequisits for the dataloaders:\n",
    "1) all files should be the same length. If there are 2610 values in one file, all files should have 2610.\n",
    "2) each dataloader should have at least one trace mapped per digital value. I am not completely sure on the impact of selecting less than 256 traces per dataloader, but based on previous attempts on using partial data, I am guessing it is necessary.\n",
    "3) a good ratio between training and testing is 7:3. We will need more data to make this possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "pwd = os.getcwd()\n",
    "trace_folder_dir = os.path.join(pwd, 'trace_files')\n",
    "if sub_folder == True:\n",
    "    trace_folder_dir = os.path.join(trace_folder_dir, sub_folder_name)\n",
    "\n",
    "train_builder = TraceDatasetBuilder(adc_bitwidth=8, cache=True)\n",
    "test_builder = TraceDatasetBuilder(adc_bitwidth=8, cache=True)\n",
    "\n",
    "# Create TRAINING dataloader\n",
    "try:\n",
    "    for folder_name in train_trace_folder_names:\n",
    "        train_builder.add_files(os.path.join(trace_folder_dir, folder_name), file_pattern, 0)\n",
    "except FileNotFoundError:\n",
    "    print(f\"The folder does not exist: {folder_name}\")\n",
    "    print(f\"Check the following settings and try again:\")\n",
    "    print(f\"\\tsub_folder: {sub_folder}\")\n",
    "    print(f\"\\tsub_folder_name: {sub_folder_name}\")\n",
    "    print(f\"\\ttrain_trace_folder_names: {train_trace_folder_names}\")\n",
    "    print(f\"\\ttest_trace_folder_names: {test_trace_folder_names}\")\n",
    "    print(f\"\\tfile_pattern: {file_pattern}\")\n",
    "    raise\n",
    "train_builder.build()\n",
    "print(len(train_builder.datasets[0]))\n",
    "train_builder.build_dataloaders(batch_size=train_batch, shuffle=True)\n",
    "\n",
    "# Create TESTING dataloader\n",
    "try:\n",
    "    for folder_name in test_trace_folder_names:\n",
    "        test_builder.add_files(os.path.join(trace_folder_dir, folder_name), file_pattern, 0)\n",
    "except FileNotFoundError:\n",
    "    print(f\"The folder does not exist: {folder_name}\")\n",
    "    print(f\"Check the following settings and try again:\")\n",
    "    print(f\"\\tsub_folder: {sub_folder}\")\n",
    "    print(f\"\\tsub_folder_name: {sub_folder_name}\")\n",
    "    print(f\"\\ttrain_trace_folder_names: {train_trace_folder_names}\")\n",
    "    print(f\"\\ttest_trace_folder_names: {test_trace_folder_names}\")\n",
    "    print(f\"\\tfile_pattern: {file_pattern}\")\n",
    "    raise\n",
    "test_builder.build()\n",
    "test_builder.build_dataloaders(batch_size=train_batch, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training CNNs - Native CNN\n",
    "Using a native CNN, we train each CNN until all of them reaches an accuracy of 1.0.\n",
    "\n",
    "I'm not sure if aiming for an accuracy of 1.0 is beneficial, as it is just overfitting the model to the training data. A more realistic value may be 0.99, but will set it to 1.0 for the current simulated environment.\n",
    "\n",
    "The training function automatically reduces the learning rate used in Adam based on target accuracy. Currently testing different values and decrease rates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "1) def_lr: Default learning rate. Change this value to test different learning rate values.\n",
    "2) max_epoch: Limit of maximum training epochs; training ends if it reaches this amount of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main hyperparameters\n",
    "def_lr = 1e-4\n",
    "max_epoch = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for optimal CNN parameters\n",
    "\n",
    "Paramerize components of CNN and find best overall hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import hashlib\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import re\n",
    "\n",
    "from copy import copy\n",
    "\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Hyperparameter format\n",
    "\n",
    "1. Convolutional Layer (Conv1d)\n",
    "Format:\n",
    "```\n",
    "conv_[# of conv layer]_[# of out_channels]_[kernel_size #]_[stride #]_[padding #]_[dilation #]\n",
    "```\n",
    "Example:\n",
    "```python\n",
    "self.conv1 = nn.Conv1d(in_channels=1, out_channels=8, kernel_size=5, stride=1, padding=2, dilation=1)\n",
    "# Naming: conv_1_1_8_5_1_2_1\n",
    "```\n",
    "NOTE: Padding mode = `'zeros'`\n",
    "\n",
    "2. Max Pooling Layer (MaxPool1d)\n",
    "Format:\n",
    "```\n",
    "maxpool_[# of pool layer]_[kernel_size #]_[stride #]_[padding #]_[dilation #]\n",
    "```\n",
    "Example:\n",
    "```python\n",
    "self.pool1 = nn.MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1)\n",
    "# Naming: maxpool_1_3_3_0_1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "3. Fully Connected Layer (Linear)\n",
    "Format:\n",
    "```\n",
    "linear_[# of fc layer]_[in_features #]_[out_features #]\n",
    "```\n",
    "Example:\n",
    "```python\n",
    "self.fc1 = nn.Linear(500, 500)\n",
    "# Naming: linear_1_500_500\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter update testing area\n",
    "# Change values within this section to attempt training on CNN\n",
    "\n",
    "# default for both conv and maxpool: padding=0, dilation=1\n",
    "# conv_[# of conv layer]_[# of out_channels]_[kernel_size #]_[stride #]_[padding #]_[dilation #]\n",
    "# maxpool_[# of pool layer]_[kernel_size #]_[stride #]_[padding #]_[dilation #]\n",
    "# linear_[# of fc layer]_[in_features #]_[out_features #]\n",
    "conv_param = \"conv_2_5_3_1_0_1\"\n",
    "maxpool_param = \"maxpool_5_5_0_1\"\n",
    "linear_param = \"linear_3_100_100\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating CNNs based on defined parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicCNN(nn.Module):\n",
    "    def __init__(self, conv_params, maxpool_params, linear_params, input_length=3001):\n",
    "        super(DynamicCNN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        in_channels = 1  # Initial input channel\n",
    "        final_size = input_length\n",
    "        # print(final_size)\n",
    "\n",
    "        # Parse convolutional layer parameters\n",
    "        try:\n",
    "            conv_layer_num, out_channels, conv_kernel_size, conv_stride, conv_padding, conv_dilation = map(int, conv_params.split(\"_\")[1:])\n",
    "        except ValueError:\n",
    "            raise ValueError(\"Error: conv_params format is incorrect.\")\n",
    "\n",
    "        # Parse max-pooling layer parameters\n",
    "        try:\n",
    "            pool_kernel_size, pool_stride, pool_padding, pool_dilation = map(int, maxpool_params.split(\"_\")[1:])\n",
    "        except ValueError:\n",
    "            raise ValueError(\"Error: maxpool_params format is incorrect.\")\n",
    "\n",
    "        # Convolutional layers\n",
    "        for _ in range(conv_layer_num):\n",
    "            self.layers.append(nn.Conv1d(in_channels, out_channels, conv_kernel_size, conv_stride, conv_padding, conv_dilation))\n",
    "            final_size = (final_size + 2 * conv_padding - conv_dilation * (conv_kernel_size - 1) - 1) // conv_stride + 1\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.MaxPool1d(pool_kernel_size, pool_stride, pool_padding, pool_dilation))\n",
    "            final_size = (final_size + 2 * pool_padding - pool_dilation * (pool_kernel_size - 1) - 1) // pool_stride + 1\n",
    "            in_channels = out_channels  # Update input channels for the next layer\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        try:\n",
    "            fc_layer_num, in_features, out_features = map(int, linear_params.split(\"_\")[1:])\n",
    "        except ValueError:\n",
    "            print(\"Error: linear_params format is incorrect.\")\n",
    "            raise\n",
    "        # First FC has in features: flat_size * out_channels\n",
    "        self.fc_layers.append(nn.Linear(out_channels * final_size, out_features))\n",
    "        self.fc_layers.append(nn.ReLU())\n",
    "        for i in range(1,int(fc_layer_num)):\n",
    "            self.fc_layers.append(nn.Linear(in_features, out_features))\n",
    "            self.fc_layers.append(nn.ReLU())\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(out_features, 2)  # Binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        try:\n",
    "            x = x.unsqueeze(1)  # Add channel dimension\n",
    "\n",
    "            # Pass through convolutional layers\n",
    "            for layer in self.layers:\n",
    "                x = layer(x)\n",
    "\n",
    "            # Flatten dynamically\n",
    "            x = x.view(x.size(0), -1)  # Convert to (batch_size, flattened_features)\n",
    "\n",
    "            # Fully connected layers\n",
    "            for layer in self.fc_layers:\n",
    "                x = layer(x)\n",
    "\n",
    "            x = torch.softmax(self.output_layer(x), dim=1)\n",
    "            return x\n",
    "        except RuntimeError:\n",
    "            print(\"Invalid form, skipping now...\")\n",
    "            x = torch.tensor([1]).float()\n",
    "            x[x == 1] = float('nan')\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Function that initializes the CNN and its core components.\n",
    "Inputs:\n",
    "    1) cnn_num: cnn bit number\n",
    "Returns:\n",
    "    1) cnn: model used(ResNet18)\n",
    "    2) criterion: loss function, current default=CrossEntropyLoss\n",
    "    3) learning_rate: learning rate, current default=def_lr\n",
    "    4) optimizer: optimizer, current default=Adam\n",
    "'''\n",
    "def cnn_init(conv_param, maxpool_param, linear_param, learning_rate, trace_len):\n",
    "    new_cnn = DynamicCNN(conv_param, maxpool_param, linear_param, trace_len)\n",
    "    # Optimizer: not specified in paper, using Adam\n",
    "    optimizer = optim.Adam(new_cnn.parameters(), lr=learning_rate)\n",
    "    # Loss function: not specified in paper, using Cross Entropy Loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Learning rate: default set to def_lr, adjust accordingly\n",
    "    learning_rate = def_lr\n",
    "    return new_cnn, criterion, optimizer\n",
    " \n",
    "''' \n",
    "Function that initializes parameters used in training.\n",
    "Inputs:\n",
    "    None\n",
    "Returns:\n",
    "    TRAINING PARAMETERS\n",
    "    1) num_epochs: number of maximum epochs per training\n",
    "    2) max_grad_norm: gradient clipping threshold\n",
    "'''\n",
    "def param_init():\n",
    "    # training parameters\n",
    "    num_epochs = max_epoch\n",
    "    max_grad_norm = 1.0\n",
    "    \n",
    "    return num_epochs, max_grad_norm\n",
    "\n",
    "''' \n",
    "Function that loads variables from saved .pth file. Checks if the file is valid.\n",
    "This function simply receives the checkpoint file and checks if it is valid.\n",
    "Inputs:\n",
    "    1) checkpoint_path: string of .pth file name \n",
    "    2) device: selected device to run training\n",
    "Returns:\n",
    "    1) pth_variables: dictionary that contains received .pth variables.\n",
    "    List of variables are saved in 'load_values' array.\n",
    "'''\n",
    "def load_pth_file(checkpoint_path, device, load_values):\n",
    "    pth_variables = {}\n",
    "    # Load .pth file\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)\n",
    "        for values in load_values:\n",
    "            pth_variables[values] = checkpoint.get(values, 0)\n",
    "        return pth_variables\n",
    "    except FileNotFoundError:\n",
    "        print(f\"No checkpoint found at {checkpoint_path}. Starting from scratch.\")\n",
    "        raise\n",
    "    except KeyError as e:\n",
    "        missing_keys = {\n",
    "            'cnn_state_dict': \"CNN state dictionary\",\n",
    "            'optimizer_state_dict': \"optimizer state dictionary\",\n",
    "            'epoch': \"epoch\",\n",
    "            'reached_acc': \"recorded accuracy of current epoch\"\n",
    "        }\n",
    "        key = e.args[0]\n",
    "        if key in missing_keys:\n",
    "            if key in ['cnn_state_dict', 'optimizer_state_dict', 'epoch', 'reached_acc']:\n",
    "                print(f\"Critical error: Missing \\\"{key}\\\" that stores \\\"{missing_keys[key]}\\\". Checkpoint file is invalid.\")\n",
    "                raise\n",
    "\n",
    "        print(f\"Unknown parameter \\\"{key}\\\" in checkpoint. Recheck file and try again.\")\n",
    "        raise\n",
    "\n",
    "# Used for plotting over certain window of values\n",
    "def moving_average(values, window=10):\n",
    "    if len(values) < window:\n",
    "        return values\n",
    "    return np.convolve(values, np.ones(window)/window, mode='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnns = []\n",
    "train_dataloaders = train_builder.dataloaders\n",
    "for a, b in train_dataloaders[0]:\n",
    "    trace_len = len(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp   = datetime.datetime.now().strftime('%Y%m%d_%H%M')\n",
    "\n",
    "print(f\"Installed CUDA version: {torch.version.cuda}\\n\")\n",
    "\n",
    "# Create CNN per dataset\n",
    "for i in range(adc_num-1, -1, -1):\n",
    "    for j in range(adc_bitwidth-1, -1, -1):\n",
    "        print(f\"Starting training for \\\"bit_{i*8+j}\\\"...\")\n",
    "\n",
    "        # initialize CNN\n",
    "        learning_rate = def_lr\n",
    "        cnn, criterion, optimizer = cnn_init(conv_param, maxpool_param, linear_param, learning_rate, trace_len)\n",
    "        # Print LR\n",
    "        print(f\"Learning rate:{learning_rate}\")\n",
    "        # initialize parameters\n",
    "        num_epochs, max_grad_norm = param_init()\n",
    "\n",
    "        # Set device to cuda if available\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"\\tGPU found, running training on GPU...\")\n",
    "            device = torch.device(\"cuda\")\n",
    "            cnn = cnn.to(device)\n",
    "        else:\n",
    "            print(\"\\tNo GPU found, running training on CPU...\")\n",
    "            print(\"\\tRecheck CUDA version and if your GPU supports it.\")\n",
    "            device = torch.device(\"cpu\")\n",
    "\n",
    "        # Append CNN to cnns array\n",
    "        cnns.append(cnn)\n",
    "\n",
    "        # Create checkpoint to save progress\n",
    "        checkpoint_path = f\"cnn_5px_adc_{i}_bit_{j}.pth\"\n",
    "\n",
    "        # Attempt to load saved .pth file\n",
    "        # start_epoch = LOCAL variable that specifies starting epoch number\n",
    "        # prev_acc = LOCAL variable that specifies accuracy achieved by loaded .pth file\n",
    "        load_values = ['cnn_state_dict', 'optimizer_state_dict', 'epoch', 'reached_acc']\n",
    "        try:\n",
    "            pth_vars = load_pth_file(checkpoint_path, device, load_values)\n",
    "        except FileNotFoundError:\n",
    "            start_epoch = 0\n",
    "            prev_acc = 0\n",
    "        else:\n",
    "            # Manually create variables per value\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "            cnn.load_state_dict(checkpoint['cnn_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            print(f\"Checkpoint loaded. Resuming from epoch {start_epoch}\")\n",
    "            prev_acc = checkpoint['reached_acc']\n",
    "        \n",
    "            print(f\"Checkpoint loaded. Resuming from epoch {start_epoch}.\")\n",
    "            print(f\"\\tPrevious reached accuracy: {prev_acc}.\")\n",
    "\n",
    "        # Skip training if target accuracy is reached\n",
    "        if prev_acc == 1:\n",
    "            print(f\"\\tSkipping training: accuracy of 1 already achieved.\\n\")\n",
    "            continue\n",
    "        \n",
    "        # Plot arrays\n",
    "        loss_values = []\n",
    "        accuracy_values = []\n",
    "        value_window = 50\n",
    "\n",
    "        plt.ion()  # Turn on interactive mode\n",
    "        fig, (ax) = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "        # Start training\n",
    "        for e in range(start_epoch, num_epochs):\n",
    "            correct = 0\n",
    "            total_traces = 0\n",
    "            cnn.train()\n",
    "            # for dataloader_index in range(i*8+j)\n",
    "            for inputs, labels in train_dataloaders[i*8+j]:\n",
    "                inputs, labels = inputs.to(device), labels.to(device) \n",
    "                optimizer.zero_grad()\n",
    "                output = cnn(inputs)\n",
    "                # Check for NaN in outputs\n",
    "                if torch.isnan(output).any():\n",
    "                    print(\"NaN detected in cnn outputs.\")\n",
    "                    break\n",
    "\n",
    "                loss = criterion(output, labels)\n",
    "                # Check for NaN in loss\n",
    "                if torch.isnan(loss):\n",
    "                    print(\"NaN detected in loss. Stopping training.\")\n",
    "                    break\n",
    "                # print(f\"Loss: {loss.item()}\")\n",
    "                loss.backward()\n",
    "\n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(cnn.parameters(), max_grad_norm)\n",
    "                optimizer.step()\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "                total_traces += len(labels)\n",
    "    \n",
    "            accuracy = correct / total_traces\n",
    "            loss_values.append(loss.item())\n",
    "            accuracy_values.append(float(accuracy))\n",
    "            \n",
    "            loss_ma = moving_average(loss_values, window=value_window)\n",
    "            accuracy_ma = moving_average(accuracy_values, window=value_window)\n",
    "\n",
    "            # Change rate of update for printing accuracy accordingly\n",
    "            if (e + 1) % 100 == 0:\n",
    "                print(f'TRAINING: cnn[{i*8+j}] (adc {i}, bit{j}), Epoch {e+1}, Loss: {loss.item()}')\n",
    "                print(f'TRAINING: cnn[{i*8+j}] (adc {i}, bit{j}), Epoch {e+1}, Accuracy: {accuracy}')\n",
    "                \n",
    "            if (e + 1) % 500 == 0:\n",
    "                # Save checkpoint\n",
    "                torch.save({\n",
    "                    'cnn_state_dict': cnn.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'epoch': e + 1,\n",
    "                    'reached_acc': accuracy\n",
    "                }, checkpoint_path)\n",
    "                print(f\"Checkpoint saved for epoch {e + 1}\")\n",
    "                clear_output(wait=True)\n",
    "                ax.clear()\n",
    "                ax.plot(range(start_epoch + 9, start_epoch + 9 + len(loss_ma)), loss_ma, label=f'Loss ({value_window}-epoch avg)', color='black')\n",
    "                ax.plot(range(start_epoch + 9, start_epoch + 9 + len(accuracy_ma)), accuracy_ma, label=f'Accuracy ({value_window}-epoch avg)', color='red')\n",
    "                ax.set_xlabel('Epoch')\n",
    "                ax.set_ylabel('Los/Accuracy')\n",
    "                ax.set_title(f'Training Loss/Accuracy ({value_window}-epoch moving average)')\n",
    "                ax.legend()\n",
    "\n",
    "                display(fig)\n",
    "                plt.pause(0.1)\n",
    "\n",
    "            prev_acc = accuracy\n",
    "\n",
    "            if accuracy >= 0.95:\n",
    "                print(f\"Reached accuracy of 1.0. Stopping training for \\\"cnn[{i*8+j}] (adc {i}, bit{j})\\\".\\n\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the CNNs\n",
    "\n",
    "Using the resulting CNNs saved in the .pth files, we test each CNNs using the provided power traces.\n",
    "\n",
    "Currently the testing data is a subset of the training data. In the future, if we can generate more data, we will be able to use separate datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Test Accuracy\n",
    "\n",
    "Prints single float value for testing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TESTING dataloaders\n",
    "test_dataloaders = test_builder.dataloaders\n",
    "track_running_stats=False\n",
    "# Run evaluation using testing data\n",
    "for i in range(adc_num-1, -1, -1):\n",
    "    for j in range(adc_bitwidth-1, -1, -1):\n",
    "        cnn = cnns[i*8+j]\n",
    "        try:\n",
    "            checkpoint_path = f\"cnn_5px_adc_{i}_bit_{j}.pth\"\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "            print(checkpoint.keys())\n",
    "            cnn.load_state_dict(checkpoint['cnn_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            print(f\"Checkpoint loaded. Resuming from epoch {start_epoch}\")\n",
    "            reached_target_acc = checkpoint['reached_acc']\n",
    "            print(f\"Previous reached accuracy: {reached_target_acc}\")\n",
    "            '''\n",
    "            if reached_target_acc != 1:\n",
    "                print(f\"cnn_{i}\\\" did not reach accuracy of 1, skipping evaluation.\\n\\n\")\n",
    "                continue\n",
    "            '''\n",
    "        except FileNotFoundError:\n",
    "            print(f\"No checkpoint found for \\\"cnn_{i}\\\". Skipping evaluation.\")\n",
    "        \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            # Use TEST dataloader\n",
    "            for inputs, labels in test_dataloaders[i*8+j]:\n",
    "                # inputs = inputs.unsqueeze(1).unsqueeze(-1)\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = cnn(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f\"Accuracy: {100 * correct / total:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification report\n",
    "\n",
    "Creates a classification report on all 8 bits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, RocCurveDisplay\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "all_labels = []\n",
    "all_logits = []  # To store raw logits\n",
    "\n",
    "for i in range(adc_num-1, -1, -1):\n",
    "    for j in range(adc_bitwidth-1, -1, -1):\n",
    "        cnn = cnns[i*8+j]\n",
    "        print(f\"{i*8+j}\")\n",
    "        # Loss function: not specified in paper, using Cross Entropy Loss\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(cnn.parameters(), lr=5e-3)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "            cnn = cnn.to(device)\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "\n",
    "        checkpoint_path = f\"resnet18_checkpoint_adc_{i}_bit_{j}.pth\"\n",
    "        try:\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "            cnn.load_state_dict(checkpoint['cnn_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            # print(f\"Checkpoint loaded. Resuming from epoch {start_epoch}\")\n",
    "            reached_target_acc = checkpoint['reached_acc']\n",
    "            # print(f\"Previous reached accuracy: {reached_target_acc}\")\n",
    "            '''\n",
    "            if reached_target_acc != 1:\n",
    "                print(f\"cnn_{i}\\\" did not reach accuracy of 1, skipping evaluation.\\n\\n\")\n",
    "                continue\n",
    "            '''\n",
    "        except FileNotFoundError:\n",
    "            print(f\"No checkpoint found for \\\"cnn_{i}\\\". Skipping evaluation.\")\n",
    "            continue\n",
    "\n",
    "        # Evaluate the model\n",
    "        with torch.no_grad():\n",
    "            # Use TEST dataloader\n",
    "            for inputs, labels in test_dataloaders[i*8+j]:\n",
    "                # inputs = inputs.unsqueeze(1).unsqueeze(-1)\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = cnn(inputs)  # Raw logits\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                adc_logits = outputs.cpu().numpy()  # Store raw logits\n",
    "                adc_preds = preds.cpu().numpy()\n",
    "                adc_labels = labels.cpu().numpy()\n",
    "\n",
    "        all_logits.append(adc_logits)\n",
    "        all_preds.append(adc_preds)\n",
    "        all_labels.append(adc_labels)\n",
    "    \n",
    "        # Compute accuracy for adc_i\n",
    "        accuracy = accuracy_score(adc_labels, adc_preds)\n",
    "        print(f\"Test Accuracy for adc_{i}: {accuracy:.4f}\")\n",
    "\n",
    "        # Generate classification report for adc_i\n",
    "        print(f\"\\nClassification Report for adc_{i}:\")\n",
    "        print(classification_report(adc_labels, adc_preds))\n",
    "\n",
    "        # Generate confusion matrix for adc_i\n",
    "        print(f\"\\nConfusion Matrix for adc_{i}:\")\n",
    "        print(confusion_matrix(adc_labels, adc_preds))\n",
    "\n",
    "all_logits = np.concatenate(all_logits)\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "\n",
    "print(\"\\nFINAL COMBINED RESULTS FOR ALL ADCS\")\n",
    "\n",
    "# Compute accuracy for all adcs\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Generate classification report for all adcs\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds))\n",
    "\n",
    "# Generate confusion matrix for all adcs\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(all_labels, all_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
